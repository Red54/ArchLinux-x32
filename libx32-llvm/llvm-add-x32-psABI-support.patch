From 7aceba1f68e2ed937c503b9170adbb8d23d2ce64 Mon Sep 17 00:00:00 2001
From: Michael Liao <michael.liao@intel.com>
Date: Tue, 1 May 2012 21:05:24 -0700
Subject: [PATCH] add x32 psABI support

- add new 'X32' environment to specify x32 mode under x86-64 target
  machine
- add ELF support for x32, i.e. ELF32 + x86-64, and fix code relying on
  ELF32/ELF64 to detect target arch and get correct reloc type
- fix pointer usage in code generator due to the pointer width change
- add LEAX32r to prepare 32-bit pointer in x32
- add LEAX32_64r to enable 64-bit arithmetic through LEA
- add PUSHX32r/POPX32r/JMPX32r/CALLX32r/CALLX32m/etc to handle special
  cases where these instructions always use 64-bit operand in 64-bit
  mode.
- add x32 JIT support
- fix autoconf/cmake to propagate '-mx32' option and detect x32 host
  target.
---
 autoconf/config.guess                              |    9 +-
 cmake/modules/GetHostTriple.cmake                  |    4 +
 include/llvm/ADT/Triple.h                          |    3 +-
 include/llvm/MC/MCELFObjectWriter.h                |    6 +-
 lib/MC/ELFObjectWriter.cpp                         |    3 +-
 lib/Support/Triple.cpp                             |    2 +
 lib/Support/Unix/Host.inc                          |    9 +
 lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp      |   18 +-
 lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp |   24 +-
 lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp       |    2 +-
 lib/Target/X86/MCTargetDesc/X86MCTargetDesc.h      |    3 +-
 lib/Target/X86/X86AsmPrinter.cpp                   |   11 +-
 lib/Target/X86/X86CallingConv.td                   |    1 +
 lib/Target/X86/X86CodeEmitter.cpp                  |   11 +-
 lib/Target/X86/X86FastISel.cpp                     |   12 +-
 lib/Target/X86/X86FrameLowering.cpp                |  212 +++++++++++------
 lib/Target/X86/X86ISelLowering.cpp                 |  250 +++++++++++++-------
 lib/Target/X86/X86ISelLowering.h                   |    3 +
 lib/Target/X86/X86InstrArithmetic.td               |   20 +-
 lib/Target/X86/X86InstrCompiler.td                 |  121 ++++++++--
 lib/Target/X86/X86InstrControl.td                  |   37 +++
 lib/Target/X86/X86InstrInfo.cpp                    |   62 +++--
 lib/Target/X86/X86InstrInfo.td                     |   31 ++-
 lib/Target/X86/X86JITInfo.cpp                      |  124 +++++++++-
 lib/Target/X86/X86MCInstLower.cpp                  |   70 +++++-
 lib/Target/X86/X86RegisterInfo.cpp                 |   42 ++--
 lib/Target/X86/X86RegisterInfo.td                  |    2 +
 lib/Target/X86/X86SelectionDAGInfo.cpp             |   24 +-
 lib/Target/X86/X86Subtarget.h                      |    6 +
 lib/Target/X86/X86TargetMachine.cpp                |    5 +-
 utils/TableGen/EDEmitter.cpp                       |    1 +
 utils/TableGen/X86RecognizableInstr.cpp            |    2 +
 32 files changed, 850 insertions(+), 280 deletions(-)

diff --git a/autoconf/config.guess b/autoconf/config.guess
index f7dd69e..45edd79 100755
--- a/autoconf/config.guess
+++ b/autoconf/config.guess
@@ -983,7 +983,14 @@ EOF
 	echo ${UNAME_MACHINE}-dec-linux-gnu
 	exit ;;
     x86_64:Linux:*:*)
-	echo x86_64-unknown-linux-gnu
+	eval $set_cc_for_build
+	if echo __ILP32__ | $CC_FOR_BUILD -E - 2>/dev/null \
+	    | grep -q __ILP32__
+	then
+	    echo x86_64-unknown-linux-gnu
+	else
+	    echo x86_64-unknown-linux-gnux32
+	fi
 	exit ;;
     xtensa*:Linux:*:*)
 	echo ${UNAME_MACHINE}-unknown-linux-gnu
diff --git a/cmake/modules/GetHostTriple.cmake b/cmake/modules/GetHostTriple.cmake
index 671a8ce..f9b038a 100644
--- a/cmake/modules/GetHostTriple.cmake
+++ b/cmake/modules/GetHostTriple.cmake
@@ -16,10 +16,14 @@ function( get_host_triple var )
     endif()
   else( MSVC )
     set(config_guess ${LLVM_MAIN_SRC_DIR}/autoconf/config.guess)
+    set(OLD_ENV_CC "$ENV{CC}")
+    # Propagate the 1st argument into subprocess as well.
+    set(ENV{CC} "${CMAKE_C_COMPILER} ${CMAKE_C_COMPILER_ARG1}")
     execute_process(COMMAND sh ${config_guess}
       RESULT_VARIABLE TT_RV
       OUTPUT_VARIABLE TT_OUT
       OUTPUT_STRIP_TRAILING_WHITESPACE)
+    set(ENV{CC} "${OLD_ENV_CC}")
     if( NOT TT_RV EQUAL 0 )
       message(FATAL_ERROR "Failed to execute ${config_guess}")
     endif( NOT TT_RV EQUAL 0 )
diff --git a/include/llvm/ADT/Triple.h b/include/llvm/ADT/Triple.h
index 6b11c7a..a73c2ba 100644
--- a/include/llvm/ADT/Triple.h
+++ b/include/llvm/ADT/Triple.h
@@ -108,7 +108,8 @@ public:
     GNUEABIHF,
     EABI,
     MachO,
-    ANDROIDEABI
+    ANDROIDEABI,
+    X32
   };
 
 private:
diff --git a/include/llvm/MC/MCELFObjectWriter.h b/include/llvm/MC/MCELFObjectWriter.h
index f153cb0..124d243 100644
--- a/include/llvm/MC/MCELFObjectWriter.h
+++ b/include/llvm/MC/MCELFObjectWriter.h
@@ -91,9 +91,9 @@ public:
 
   /// @name Accessors
   /// @{
-  uint8_t getOSABI() { return OSABI; }
-  uint16_t getEMachine() { return EMachine; }
-  bool hasRelocationAddend() { return HasRelocationAddend; }
+  uint8_t getOSABI() const { return OSABI; }
+  uint16_t getEMachine() const { return EMachine; }
+  bool hasRelocationAddend() const { return HasRelocationAddend; }
   bool is64Bit() const { return Is64Bit; }
   /// @}
 };
diff --git a/lib/MC/ELFObjectWriter.cpp b/lib/MC/ELFObjectWriter.cpp
index 7b166fb..4a9aab7 100644
--- a/lib/MC/ELFObjectWriter.cpp
+++ b/lib/MC/ELFObjectWriter.cpp
@@ -135,6 +135,7 @@ class ELFObjectWriter : public MCObjectWriter {
     }
 
     bool is64Bit() const { return TargetObjectWriter->is64Bit(); }
+    uint16_t getEMachine() const { return TargetObjectWriter->getEMachine(); }
     bool hasRelocationAddend() const {
       return TargetObjectWriter->hasRelocationAddend();
     }
@@ -733,7 +734,7 @@ void ELFObjectWriter::RecordRelocation(const MCAssembler &Asm,
     }
     Addend = Value;
     // Compensate for the addend on i386.
-    if (is64Bit())
+    if (hasRelocationAddend())
       Value = 0;
   }
 
diff --git a/lib/Support/Triple.cpp b/lib/Support/Triple.cpp
index 822ada7..5a74e43 100644
--- a/lib/Support/Triple.cpp
+++ b/lib/Support/Triple.cpp
@@ -133,6 +133,7 @@ const char *Triple::getEnvironmentTypeName(EnvironmentType Kind) {
   case EABI: return "eabi";
   case MachO: return "macho";
   case ANDROIDEABI: return "androideabi";
+  case X32: return "gnux32";
   }
 
   llvm_unreachable("Invalid EnvironmentType!");
@@ -296,6 +297,7 @@ static Triple::EnvironmentType parseEnvironment(StringRef EnvironmentName) {
     .StartsWith("eabi", Triple::EABI)
     .StartsWith("gnueabihf", Triple::GNUEABIHF)
     .StartsWith("gnueabi", Triple::GNUEABI)
+    .StartsWith("gnux32", Triple::X32)
     .StartsWith("gnu", Triple::GNU)
     .StartsWith("macho", Triple::MachO)
     .StartsWith("androideabi", Triple::ANDROIDEABI)
diff --git a/lib/Support/Unix/Host.inc b/lib/Support/Unix/Host.inc
index 726e2fb..6bb13e1 100644
--- a/lib/Support/Unix/Host.inc
+++ b/lib/Support/Unix/Host.inc
@@ -59,5 +59,14 @@ std::string sys::getDefaultTargetTriple() {
     Triple += getOSVersion();
   }
 
+#if defined(__x86_64__) && defined(__ILP32__)
+  // Append 'x32' environment on x86-64 if applicable.
+  std::string::size_type X32DashIdx = Triple.find("-gnu");
+  if (X32DashIdx != std::string::npos) {
+    Triple.resize(X32DashIdx + strlen("-gnu"));
+    Triple += "x32";
+  }
+#endif
+
   return Triple;
 }
diff --git a/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp b/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp
index 32e40fe..b892a13 100644
--- a/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp
+++ b/lib/Target/X86/MCTargetDesc/X86AsmBackend.cpp
@@ -344,7 +344,7 @@ public:
     : ELFX86AsmBackend(T, OSABI) {}
 
   MCObjectWriter *createObjectWriter(raw_ostream &OS) const {
-    return createX86ELFObjectWriter(OS, /*Is64Bit*/ false, OSABI);
+    return createX86ELFObjectWriter(OS, /*Is64Bit*/ false, OSABI, ELF::EM_386);
   }
 };
 
@@ -354,7 +354,17 @@ public:
     : ELFX86AsmBackend(T, OSABI) {}
 
   MCObjectWriter *createObjectWriter(raw_ostream &OS) const {
-    return createX86ELFObjectWriter(OS, /*Is64Bit*/ true, OSABI);
+    return createX86ELFObjectWriter(OS, /*Is64Bit*/ true, OSABI, ELF::EM_X86_64);
+  }
+};
+
+class ELFX86_X32AsmBackend : public ELFX86AsmBackend {
+public:
+  ELFX86_X32AsmBackend(const Target &T, uint8_t OSABI)
+    : ELFX86AsmBackend(T, OSABI) {}
+
+  MCObjectWriter *createObjectWriter(raw_ostream &OS) const {
+    return createX86ELFObjectWriter(OS, /*Is64Bit*/ false, OSABI, ELF::EM_X86_64);
   }
 };
 
@@ -462,5 +472,9 @@ MCAsmBackend *llvm::createX86_64AsmBackend(const Target &T, StringRef TT) {
     return new WindowsX86AsmBackend(T, true);
 
   uint8_t OSABI = MCELFObjectTargetWriter::getOSABI(TheTriple.getOS());
+
+  if (TheTriple.getEnvironment() == Triple::X32)
+    return new ELFX86_X32AsmBackend(T, OSABI);
+
   return new ELFX86_64AsmBackend(T, OSABI);
 }
diff --git a/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp b/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp
index 5a42a80..1d0ba48 100644
--- a/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp
+++ b/lib/Target/X86/MCTargetDesc/X86ELFObjectWriter.cpp
@@ -20,7 +20,7 @@ using namespace llvm;
 namespace {
   class X86ELFObjectWriter : public MCELFObjectTargetWriter {
   public:
-    X86ELFObjectWriter(bool is64Bit, uint8_t OSABI);
+    X86ELFObjectWriter(bool IsELF64, uint8_t OSABI, uint16_t EMachine);
 
     virtual ~X86ELFObjectWriter();
   protected:
@@ -30,10 +30,11 @@ namespace {
   };
 }
 
-X86ELFObjectWriter::X86ELFObjectWriter(bool Is64Bit, uint8_t OSABI)
-  : MCELFObjectTargetWriter(Is64Bit, OSABI,
-                            Is64Bit ?  ELF::EM_X86_64 : ELF::EM_386,
-                            /*HasRelocationAddend*/ Is64Bit) {}
+X86ELFObjectWriter::X86ELFObjectWriter(bool IsELF64, uint8_t OSABI,
+                                       uint16_t EMachine)
+  : MCELFObjectTargetWriter(IsELF64, OSABI, EMachine,
+			    /*HasRelocationAddend*/ EMachine == ELF::EM_X86_64)
+{}
 
 X86ELFObjectWriter::~X86ELFObjectWriter()
 {}
@@ -48,7 +49,7 @@ unsigned X86ELFObjectWriter::GetRelocType(const MCValue &Target,
   MCSymbolRefExpr::VariantKind Modifier = Target.isAbsolute() ?
     MCSymbolRefExpr::VK_None : Target.getSymA()->getKind();
   unsigned Type;
-  if (is64Bit()) {
+  if (getEMachine() == ELF::EM_X86_64) {
     if (IsPCRel) {
       switch ((unsigned)Fixup.getKind()) {
       default: llvm_unreachable("invalid fixup kind!");
@@ -130,7 +131,7 @@ unsigned X86ELFObjectWriter::GetRelocType(const MCValue &Target,
       case FK_Data_1: Type = ELF::R_X86_64_8; break;
       }
     }
-  } else {
+  } else if (getEMachine() == ELF::EM_386) {
     if (IsPCRel) {
       switch ((unsigned)Fixup.getKind()) {
       default: llvm_unreachable("invalid fixup kind!");
@@ -210,15 +211,18 @@ unsigned X86ELFObjectWriter::GetRelocType(const MCValue &Target,
       case FK_Data_1: Type = ELF::R_386_8; break;
       }
     }
+  } else {
+    llvm_unreachable("Unsupported ELF machine type.");
   }
 
   return Type;
 }
 
 MCObjectWriter *llvm::createX86ELFObjectWriter(raw_ostream &OS,
-                                               bool Is64Bit,
-                                               uint8_t OSABI) {
+                                               bool IsELF64,
+                                               uint8_t OSABI,
+                                               uint16_t EMachine) {
   MCELFObjectTargetWriter *MOTW =
-    new X86ELFObjectWriter(Is64Bit, OSABI);
+    new X86ELFObjectWriter(IsELF64, OSABI, EMachine);
   return createELFObjectWriter(MOTW, OS,  /*IsLittleEndian=*/true);
 }
diff --git a/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp b/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp
index 49c07f3..adcb1f9 100644
--- a/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp
+++ b/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp
@@ -71,7 +71,7 @@ X86_64MCAsmInfoDarwin::X86_64MCAsmInfoDarwin(const Triple &Triple)
 void X86ELFMCAsmInfo::anchor() { }
 
 X86ELFMCAsmInfo::X86ELFMCAsmInfo(const Triple &T) {
-  if (T.getArch() == Triple::x86_64)
+  if (T.getArch() == Triple::x86_64 && T.getEnvironment() != Triple::X32)
     PointerSize = 8;
 
   AssemblerDialect = AsmWriterFlavor;
diff --git a/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.h b/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.h
index 4650069..36b25c4 100644
--- a/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.h
+++ b/lib/Target/X86/MCTargetDesc/X86MCTargetDesc.h
@@ -92,7 +92,8 @@ MCObjectWriter *createX86MachObjectWriter(raw_ostream &OS,
 /// createX86ELFObjectWriter - Construct an X86 ELF object writer.
 MCObjectWriter *createX86ELFObjectWriter(raw_ostream &OS,
                                          bool Is64Bit,
-                                         uint8_t OSABI);
+                                         uint8_t OSABI,
+                                         uint16_t EMachine);
 /// createX86WinCOFFObjectWriter - Construct an X86 Win COFF object writer.
 MCObjectWriter *createX86WinCOFFObjectWriter(raw_ostream &OS, bool Is64Bit);
 } // End llvm namespace
diff --git a/lib/Target/X86/X86AsmPrinter.cpp b/lib/Target/X86/X86AsmPrinter.cpp
index d30c8df..d5ac30d 100644
--- a/lib/Target/X86/X86AsmPrinter.cpp
+++ b/lib/Target/X86/X86AsmPrinter.cpp
@@ -696,9 +696,14 @@ X86AsmPrinter::getDebugValueLocation(const MachineInstr *MI) const {
   assert (MI->getNumOperands() == 7 && "Invalid no. of machine operands!");
   // Frame address.  Currently handles register +- offset only.
 
-  if (MI->getOperand(0).isReg() && MI->getOperand(3).isImm())
-    Location.set(MI->getOperand(0).getReg(), MI->getOperand(3).getImm());
-  else {
+  if (MI->getOperand(0).isReg() && MI->getOperand(3).isImm()) {
+    if (Subtarget->isX32()) {
+      unsigned MachineReg =
+        getX86SubSuperRegister(MI->getOperand(0).getReg(), MVT::i64, false);
+      Location.set(MachineReg, MI->getOperand(3).getImm());
+    } else
+      Location.set(MI->getOperand(0).getReg(), MI->getOperand(3).getImm());
+  } else {
     DEBUG(dbgs() << "DBG_VALUE instruction ignored! " << *MI << "\n");
   }
   return Location;
diff --git a/lib/Target/X86/X86CallingConv.td b/lib/Target/X86/X86CallingConv.td
index a6d2709..2c0f9e1 100644
--- a/lib/Target/X86/X86CallingConv.td
+++ b/lib/Target/X86/X86CallingConv.td
@@ -144,6 +144,7 @@ def CC_X86_64_C : CallingConv<[
   CCIfType<[i8, i16], CCPromoteToType<i32>>,
 
   // The 'nest' parameter, if any, is passed in R10.
+  CCIfNest<CCIfSubtarget<"isX32()", CCAssignToReg<[R10D]>>>,
   CCIfNest<CCAssignToReg<[R10]>>,
 
   // The first 6 integer arguments are passed in integer registers.
diff --git a/lib/Target/X86/X86CodeEmitter.cpp b/lib/Target/X86/X86CodeEmitter.cpp
index 3079dfa..fabaca8 100644
--- a/lib/Target/X86/X86CodeEmitter.cpp
+++ b/lib/Target/X86/X86CodeEmitter.cpp
@@ -1088,6 +1088,8 @@ void Emitter<CodeEmitter>::emitInstruction(MachineInstr &MI,
                                            const MCInstrDesc *Desc) {
   DEBUG(dbgs() << MI);
 
+  bool IsX32 = TM.getSubtarget<X86Subtarget>().isX32();
+
   // If this is a pseudo instruction, lower it.
   switch (Desc->getOpcode()) {
   case X86::ADD16rr_DB:      Desc = UpdateOp(MI, II, X86::OR16rr); break;
@@ -1224,7 +1226,8 @@ void Emitter<CodeEmitter>::emitInstruction(MachineInstr &MI,
     }
 
     assert(MO.isImm() && "Unknown RawFrm operand!");
-    if (Opcode == X86::CALLpcrel32 || Opcode == X86::CALL64pcrel32) {
+    if (Opcode == X86::CALLpcrel32 || Opcode == X86::CALL64pcrel32 ||
+        Opcode == X86::CALLX32pcrel32) {
       // Fix up immediate operand for pc relative calls.
       intptr_t Imm = (intptr_t)MO.getImm();
       Imm = Imm - MCE.getCurrentPCValue() - 4;
@@ -1255,6 +1258,8 @@ void Emitter<CodeEmitter>::emitInstruction(MachineInstr &MI,
     // This should not occur on Darwin for relocatable objects.
     if (Opcode == X86::MOV64ri)
       rt = X86::reloc_absolute_dword;  // FIXME: add X86II flag?
+    if (IsX32 && Opcode == X86::MOV32ri)
+      rt = X86::reloc_absolute_word; // FIXME: add X86II flag?
     if (MO1.isGlobal()) {
       bool Indirect = gvNeedsNonLazyPtr(MO1, TM);
       emitGlobalAddress(MO1.getGlobal(), rt, MO1.getOffset(), 0,
@@ -1351,6 +1356,8 @@ void Emitter<CodeEmitter>::emitInstruction(MachineInstr &MI,
       : (IsPIC ? X86::reloc_picrel_word : X86::reloc_absolute_word);
     if (Opcode == X86::MOV64ri32)
       rt = X86::reloc_absolute_word_sext;  // FIXME: add X86II flag?
+    if (IsX32 && Opcode == X86::MOV32ri)
+      rt = X86::reloc_absolute_word; // FIXME: add X86II flag?
     if (MO1.isGlobal()) {
       bool Indirect = gvNeedsNonLazyPtr(MO1, TM);
       emitGlobalAddress(MO1.getGlobal(), rt, MO1.getOffset(), 0,
@@ -1393,6 +1400,8 @@ void Emitter<CodeEmitter>::emitInstruction(MachineInstr &MI,
       : (IsPIC ? X86::reloc_picrel_word : X86::reloc_absolute_word);
     if (Opcode == X86::MOV64mi32)
       rt = X86::reloc_absolute_word_sext;  // FIXME: add X86II flag?
+    if (IsX32 && Opcode == X86::MOV32mi)
+      rt = X86::reloc_absolute_word; // FIXME: add X86II flag?
     if (MO.isGlobal()) {
       bool Indirect = gvNeedsNonLazyPtr(MO, TM);
       emitGlobalAddress(MO.getGlobal(), rt, MO.getOffset(), 0,
diff --git a/lib/Target/X86/X86FastISel.cpp b/lib/Target/X86/X86FastISel.cpp
index 07d0e76..7acf854 100644
--- a/lib/Target/X86/X86FastISel.cpp
+++ b/lib/Target/X86/X86FastISel.cpp
@@ -1794,7 +1794,7 @@ bool X86FastISel::DoSelectCall(const Instruction *I, const char *MemIntName) {
     // Register-indirect call.
     unsigned CallOpc;
     if (Subtarget->is64Bit())
-      CallOpc = X86::CALL64r;
+      CallOpc = Subtarget->isX32() ? X86::CALLX32r : X86::CALL64r;
     else
       CallOpc = X86::CALL32r;
     MIB = BuildMI(*FuncInfo.MBB, FuncInfo.InsertPt, DL, TII.get(CallOpc))
@@ -1805,7 +1805,7 @@ bool X86FastISel::DoSelectCall(const Instruction *I, const char *MemIntName) {
     assert(GV && "Not a direct call");
     unsigned CallOpc;
     if (Subtarget->is64Bit())
-      CallOpc = X86::CALL64pcrel32;
+      CallOpc = Subtarget->isX32() ? X86::CALLX32pcrel32 : X86::CALL64pcrel32;
     else
       CallOpc = X86::CALLpcrel32;
 
@@ -2051,7 +2051,9 @@ unsigned X86FastISel::TargetMaterializeConstant(const Constant *C) {
           AM.IndexReg == 0 && AM.Disp == 0 && AM.GV == 0)
         return AM.Base.Reg;
 
-      Opc = TLI.getPointerTy() == MVT::i32 ? X86::LEA32r : X86::LEA64r;
+      Opc = TLI.getPointerTy() == MVT::i32
+            ? (Subtarget->isX32() ? X86::LEAX32r : X86::LEA32r)
+            : X86::LEA64r;
       unsigned ResultReg = createResultReg(RC);
       addFullAddress(BuildMI(*FuncInfo.MBB, FuncInfo.InsertPt, DL,
                              TII.get(Opc), ResultReg), AM);
@@ -2105,7 +2107,9 @@ unsigned X86FastISel::TargetMaterializeAlloca(const AllocaInst *C) {
   X86AddressMode AM;
   if (!X86SelectAddress(C, AM))
     return 0;
-  unsigned Opc = Subtarget->is64Bit() ? X86::LEA64r : X86::LEA32r;
+  unsigned Opc = Subtarget->is64Bit()
+                 ? (Subtarget->isX32() ? X86::LEAX32r : X86::LEA64r)
+                 : X86::LEA32r;
   const TargetRegisterClass* RC = TLI.getRegClassFor(TLI.getPointerTy());
   unsigned ResultReg = createResultReg(RC);
   addFullAddress(BuildMI(*FuncInfo.MBB, FuncInfo.InsertPt, DL,
diff --git a/lib/Target/X86/X86FrameLowering.cpp b/lib/Target/X86/X86FrameLowering.cpp
index c2b1cf7..8a1f0f5 100644
--- a/lib/Target/X86/X86FrameLowering.cpp
+++ b/lib/Target/X86/X86FrameLowering.cpp
@@ -55,8 +55,8 @@ bool X86FrameLowering::hasFP(const MachineFunction &MF) const {
           MMI.callsUnwindInit());
 }
 
-static unsigned getSUBriOpcode(unsigned is64Bit, int64_t Imm) {
-  if (is64Bit) {
+static unsigned getSUBriOpcode(unsigned IsLP64, int64_t Imm) {
+  if (IsLP64) {
     if (isInt<8>(Imm))
       return X86::SUB64ri8;
     return X86::SUB64ri32;
@@ -67,8 +67,8 @@ static unsigned getSUBriOpcode(unsigned is64Bit, int64_t Imm) {
   }
 }
 
-static unsigned getADDriOpcode(unsigned is64Bit, int64_t Imm) {
-  if (is64Bit) {
+static unsigned getADDriOpcode(unsigned IsLP64, int64_t Imm) {
+  if (IsLP64) {
     if (isInt<8>(Imm))
       return X86::ADD64ri8;
     return X86::ADD64ri32;
@@ -109,12 +109,15 @@ static unsigned findDeadCallerSavedReg(MachineBasicBlock &MBB,
   default: return 0;
   case X86::RET:
   case X86::RETI:
+  case X86::TCRETURNdiX32:
+  case X86::TCRETURNriX32:
   case X86::TCRETURNdi:
   case X86::TCRETURNri:
   case X86::TCRETURNmi:
   case X86::TCRETURNdi64:
   case X86::TCRETURNri64:
   case X86::TCRETURNmi64:
+  case X86::EH_RETURNX32:
   case X86::EH_RETURN:
   case X86::EH_RETURN64: {
     SmallSet<uint16_t, 8> Uses;
@@ -145,7 +148,7 @@ static unsigned findDeadCallerSavedReg(MachineBasicBlock &MBB,
 static
 void emitSPUpdate(MachineBasicBlock &MBB, MachineBasicBlock::iterator &MBBI,
                   unsigned StackPtr, int64_t NumBytes,
-                  bool Is64Bit, bool UseLEA,
+                  bool Is64Bit, bool IsLP64, bool UseLEA,
                   const TargetInstrInfo &TII, const TargetRegisterInfo &TRI) {
   bool isSub = NumBytes < 0;
   uint64_t Offset = isSub ? -NumBytes : NumBytes;
@@ -154,8 +157,8 @@ void emitSPUpdate(MachineBasicBlock &MBB, MachineBasicBlock::iterator &MBBI,
     Opc = getLEArOpcode(Is64Bit);
   else
     Opc = isSub
-      ? getSUBriOpcode(Is64Bit, Offset)
-      : getADDriOpcode(Is64Bit, Offset);
+      ? getSUBriOpcode(IsLP64, Offset)
+      : getADDriOpcode(IsLP64, Offset);
 
   uint64_t Chunk = (1LL << 31) - 1;
   DebugLoc DL = MBB.findDebugLoc(MBBI);
@@ -313,11 +316,11 @@ void X86FrameLowering::emitCalleeSavedFrameMoves(MachineFunction &MF,
   if (CSI.empty()) return;
 
   std::vector<MachineMove> &Moves = MMI.getFrameMoves();
-  const TargetData *TD = TM.getTargetData();
+  const X86RegisterInfo *RegInfo = TM.getRegisterInfo();
   bool HasFP = hasFP(MF);
 
   // Calculate amount of bytes used for return address storing.
-  int stackGrowth = -TD->getPointerSize();
+  int stackGrowth = -RegInfo->getSlotSize();
 
   // FIXME: This is dirty hack. The code itself is pretty mess right now.
   // It should be rewritten from scratch and generalized sometimes.
@@ -360,8 +363,10 @@ void X86FrameLowering::emitCalleeSavedFrameMoves(MachineFunction &MF,
     if (HasFP && FramePtr == Reg)
       continue;
 
+    unsigned MachineReg = STI.isX32()
+      ? getX86SubSuperRegister(Reg, MVT::i64, false) : Reg;
     MachineLocation CSDst(MachineLocation::VirtualFP, Offset);
-    MachineLocation CSSrc(Reg);
+    MachineLocation CSSrc(MachineReg);
     Moves.push_back(MachineMove(Label, CSDst, CSSrc));
   }
 }
@@ -506,11 +511,13 @@ uint32_t X86FrameLowering::getCompactUnwindEncoding(MachineFunction &MF) const {
 
   unsigned OffsetSize = (Is64Bit ? 8 : 4);
 
-  unsigned PushInstr = (Is64Bit ? X86::PUSH64r : X86::PUSH32r);
+  unsigned PushInstr = (Is64Bit
+                        ? (STI.isLP64() ? X86::PUSH64r : X86::PUSHX32r)
+                        : X86::PUSH32r);
   unsigned PushInstrSize = 1;
-  unsigned MoveInstr = (Is64Bit ? X86::MOV64rr : X86::MOV32rr);
-  unsigned MoveInstrSize = (Is64Bit ? 3 : 2);
-  unsigned SubtractInstrIdx = (Is64Bit ? 3 : 2);
+  unsigned MoveInstr = (STI.isLP64() ? X86::MOV64rr : X86::MOV32rr);
+  unsigned MoveInstrSize = (STI.isLP64() ? 3 : 2);
+  unsigned SubtractInstrIdx = (STI.isLP64() ? 3 : 2);
 
   unsigned StackDivide = (Is64Bit ? 8 : 4);
 
@@ -541,7 +548,8 @@ uint32_t X86FrameLowering::getCompactUnwindEncoding(MachineFunction &MF) const {
       unsigned SrcReg = MI.getOperand(1).getReg();
       unsigned DstReg = MI.getOperand(0).getReg();
 
-      if (DstReg != FramePtr || SrcReg != StackPtr)
+      if (!RegInfo->regsOverlap(DstReg, FramePtr) ||
+          !RegInfo->regsOverlap(SrcReg, StackPtr))
         return 0;
 
       StackAdjust = 0;
@@ -650,6 +658,10 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
   unsigned SlotSize = RegInfo->getSlotSize();
   unsigned FramePtr = RegInfo->getFrameRegister(MF);
   unsigned StackPtr = RegInfo->getStackRegister();
+  unsigned MachineFramePtr = STI.isX32() ?
+    getX86SubSuperRegister(FramePtr, MVT::i64, false) : FramePtr;
+  unsigned MachineStackPtr = STI.isX32() ?
+    getX86SubSuperRegister(StackPtr, MVT::i64, false) : StackPtr;
   DebugLoc DL;
 
   // If we're forcing a stack realignment we can't rely on just the frame
@@ -691,7 +703,7 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
   if (TailCallReturnAddrDelta < 0) {
     MachineInstr *MI =
       BuildMI(MBB, MBBI, DL,
-              TII.get(getSUBriOpcode(Is64Bit, -TailCallReturnAddrDelta)),
+              TII.get(getSUBriOpcode(STI.isLP64(), -TailCallReturnAddrDelta)),
               StackPtr)
         .addReg(StackPtr)
         .addImm(-TailCallReturnAddrDelta)
@@ -714,9 +726,8 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
   //        ELSE                        => DW_CFA_offset_extended
 
   std::vector<MachineMove> &Moves = MMI.getFrameMoves();
-  const TargetData *TD = MF.getTarget().getTargetData();
   uint64_t NumBytes = 0;
-  int stackGrowth = -TD->getPointerSize();
+  int stackGrowth = -SlotSize;
 
   if (HasFP) {
     // Calculate required stack adjustment.
@@ -732,7 +743,10 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
     MFI->setOffsetAdjustment(-NumBytes);
 
     // Save EBP/RBP into the appropriate stack slot.
-    BuildMI(MBB, MBBI, DL, TII.get(Is64Bit ? X86::PUSH64r : X86::PUSH32r))
+    BuildMI(MBB, MBBI, DL, TII.get(Is64Bit
+                                   ? (STI.isX32() ? X86::PUSHX32r :
+                                                    X86::PUSH64r)
+                                   : X86::PUSH32r))
       .addReg(FramePtr, RegState::Kill)
       .setMIFlag(MachineInstr::FrameSetup);
 
@@ -748,20 +762,20 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
         MachineLocation SPSrc(MachineLocation::VirtualFP, 2 * stackGrowth);
         Moves.push_back(MachineMove(FrameLabel, SPDst, SPSrc));
       } else {
-        MachineLocation SPDst(StackPtr);
-        MachineLocation SPSrc(StackPtr, stackGrowth);
+        MachineLocation SPDst(MachineStackPtr);
+        MachineLocation SPSrc(MachineStackPtr, stackGrowth);
         Moves.push_back(MachineMove(FrameLabel, SPDst, SPSrc));
       }
 
       // Change the rule for the FramePtr to be an "offset" rule.
       MachineLocation FPDst(MachineLocation::VirtualFP, 2 * stackGrowth);
-      MachineLocation FPSrc(FramePtr);
+      MachineLocation FPSrc(MachineFramePtr);
       Moves.push_back(MachineMove(FrameLabel, FPDst, FPSrc));
     }
 
     // Update EBP with the new base value.
     BuildMI(MBB, MBBI, DL,
-            TII.get(Is64Bit ? X86::MOV64rr : X86::MOV32rr), FramePtr)
+            TII.get(STI.isLP64() ? X86::MOV64rr : X86::MOV32rr), FramePtr)
         .addReg(StackPtr)
         .setMIFlag(MachineInstr::FrameSetup);
 
@@ -772,7 +786,7 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
         .addSym(FrameLabel);
 
       // Define the current CFA to use the EBP/RBP register.
-      MachineLocation FPDst(FramePtr);
+      MachineLocation FPDst(MachineFramePtr);
       MachineLocation FPSrc(MachineLocation::VirtualFP);
       Moves.push_back(MachineMove(FrameLabel, FPDst, FPSrc));
     }
@@ -786,7 +800,7 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
     if (RegInfo->needsStackRealignment(MF)) {
       MachineInstr *MI =
         BuildMI(MBB, MBBI, DL,
-                TII.get(Is64Bit ? X86::AND64ri32 : X86::AND32ri), StackPtr)
+                TII.get(STI.isLP64() ? X86::AND64ri32 : X86::AND32ri), StackPtr)
         .addReg(StackPtr)
         .addImm(-MaxAlign)
         .setMIFlag(MachineInstr::FrameSetup);
@@ -804,7 +818,8 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
 
   while (MBBI != MBB.end() &&
          (MBBI->getOpcode() == X86::PUSH32r ||
-          MBBI->getOpcode() == X86::PUSH64r)) {
+          MBBI->getOpcode() == X86::PUSH64r ||
+          MBBI->getOpcode() == X86::PUSHX32r)) {
     PushedRegs = true;
     MBBI->setFlag(MachineInstr::FrameSetup);
     ++MBBI;
@@ -815,7 +830,7 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
       BuildMI(MBB, MBBI, DL, TII.get(X86::PROLOG_LABEL)).addSym(Label);
 
       // Define the current CFA rule to use the provided offset.
-      unsigned Ptr = StackSize ? MachineLocation::VirtualFP : StackPtr;
+      unsigned Ptr = StackSize ? MachineLocation::VirtualFP : MachineStackPtr;
       MachineLocation SPDst(Ptr);
       MachineLocation SPSrc(Ptr, StackOffset);
       Moves.push_back(MachineMove(Label, SPDst, SPSrc));
@@ -899,7 +914,7 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
     // FIXME: %rax preserves the offset and should be available.
     if (isSPUpdateNeeded)
       emitSPUpdate(MBB, MBBI, StackPtr, -(int64_t)NumBytes, Is64Bit,
-                   UseLEA, TII, *RegInfo);
+                   STI.isLP64(), UseLEA, TII, *RegInfo);
 
     if (isEAXAlive) {
         // Restore EAX
@@ -911,7 +926,7 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
     }
   } else if (NumBytes)
     emitSPUpdate(MBB, MBBI, StackPtr, -(int64_t)NumBytes, Is64Bit,
-                 UseLEA, TII, *RegInfo);
+                 STI.isLP64(), UseLEA, TII, *RegInfo);
 
   if (( (!HasFP && NumBytes) || PushedRegs) && needsFrameMoves) {
     // Mark end of stack pointer adjustment.
@@ -927,8 +942,8 @@ void X86FrameLowering::emitPrologue(MachineFunction &MF) const {
                               -StackSize + stackGrowth);
         Moves.push_back(MachineMove(Label, SPDst, SPSrc));
       } else {
-        MachineLocation SPDst(StackPtr);
-        MachineLocation SPSrc(StackPtr, stackGrowth);
+        MachineLocation SPDst(MachineStackPtr);
+        MachineLocation SPSrc(MachineStackPtr, stackGrowth);
         Moves.push_back(MachineMove(Label, SPDst, SPSrc));
       }
     }
@@ -966,12 +981,15 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
     llvm_unreachable("Can only insert epilog into returning blocks");
   case X86::RET:
   case X86::RETI:
+  case X86::TCRETURNdiX32:
+  case X86::TCRETURNriX32:
   case X86::TCRETURNdi:
   case X86::TCRETURNri:
   case X86::TCRETURNmi:
   case X86::TCRETURNdi64:
   case X86::TCRETURNri64:
   case X86::TCRETURNmi64:
+  case X86::EH_RETURNX32:
   case X86::EH_RETURN:
   case X86::EH_RETURN64:
     break;  // These are ok
@@ -1004,7 +1022,9 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
 
     // Pop EBP.
     BuildMI(MBB, MBBI, DL,
-            TII.get(Is64Bit ? X86::POP64r : X86::POP32r), FramePtr);
+            TII.get(Is64Bit
+                    ? (STI.isX32() ? X86::POPX32r : X86::POP64r)
+                    : X86::POP32r), FramePtr);
   } else {
     NumBytes = StackSize - CSSize;
   }
@@ -1015,8 +1035,8 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
     MachineBasicBlock::iterator PI = prior(MBBI);
     unsigned Opc = PI->getOpcode();
 
-    if (Opc != X86::POP32r && Opc != X86::POP64r && Opc != X86::DBG_VALUE &&
-        !PI->isTerminator())
+    if (Opc != X86::POP32r && Opc != X86::POP64r && Opc != X86::POPX32r &&
+        Opc != X86::DBG_VALUE && !PI->isTerminator())
       break;
 
     --MBBI;
@@ -1036,33 +1056,36 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
     // We cannot use LEA here, because stack pointer was realigned. We need to
     // deallocate local frame back.
     if (CSSize) {
-      emitSPUpdate(MBB, MBBI, StackPtr, NumBytes, Is64Bit, UseLEA, TII,
-                   *RegInfo);
+      emitSPUpdate(MBB, MBBI, StackPtr, NumBytes, Is64Bit, STI.isLP64(),
+                   UseLEA, TII, *RegInfo);
       MBBI = prior(LastCSPop);
     }
 
     BuildMI(MBB, MBBI, DL,
-            TII.get(Is64Bit ? X86::MOV64rr : X86::MOV32rr),
+            TII.get(STI.isLP64() ? X86::MOV64rr : X86::MOV32rr),
             StackPtr).addReg(FramePtr);
   } else if (MFI->hasVarSizedObjects()) {
     if (CSSize) {
-      unsigned Opc = Is64Bit ? X86::LEA64r : X86::LEA32r;
+      unsigned Opc = Is64Bit ? (STI.isX32() ? X86::LEAX32r : X86::LEA64r)
+                             : X86::LEA32r;
       MachineInstr *MI =
         addRegOffset(BuildMI(MF, DL, TII.get(Opc), StackPtr),
                      FramePtr, false, -CSSize);
       MBB.insert(MBBI, MI);
     } else {
       BuildMI(MBB, MBBI, DL,
-              TII.get(Is64Bit ? X86::MOV64rr : X86::MOV32rr), StackPtr)
+              TII.get(STI.isLP64() ? X86::MOV64rr : X86::MOV32rr), StackPtr)
         .addReg(FramePtr);
     }
   } else if (NumBytes) {
     // Adjust stack pointer back: ESP += numbytes.
-    emitSPUpdate(MBB, MBBI, StackPtr, NumBytes, Is64Bit, UseLEA, TII, *RegInfo);
+    emitSPUpdate(MBB, MBBI, StackPtr, NumBytes, Is64Bit, STI.isLP64(), UseLEA,
+                 TII, *RegInfo);
   }
 
   // We're returning from function via eh_return.
-  if (RetOpcode == X86::EH_RETURN || RetOpcode == X86::EH_RETURN64) {
+  if (RetOpcode == X86::EH_RETURN || RetOpcode == X86::EH_RETURN64 ||
+      RetOpcode == X86::EH_RETURNX32) {
     MBBI = MBB.getLastNonDebugInstr();
     MachineOperand &DestAddr  = MBBI->getOperand(0);
     assert(DestAddr.isReg() && "Offset should be in register!");
@@ -1072,7 +1095,8 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
   } else if (RetOpcode == X86::TCRETURNri || RetOpcode == X86::TCRETURNdi ||
              RetOpcode == X86::TCRETURNmi ||
              RetOpcode == X86::TCRETURNri64 || RetOpcode == X86::TCRETURNdi64 ||
-             RetOpcode == X86::TCRETURNmi64) {
+             RetOpcode == X86::TCRETURNmi64 ||
+             RetOpcode == X86::TCRETURNriX32 || RetOpcode == X86::TCRETURNdiX32) {
     bool isMem = RetOpcode == X86::TCRETURNmi || RetOpcode == X86::TCRETURNmi64;
     // Tail call return: adjust the stack pointer and jump to callee.
     MBBI = MBB.getLastNonDebugInstr();
@@ -1093,14 +1117,18 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
     if (Offset) {
       // Check for possible merge with preceding ADD instruction.
       Offset += mergeSPUpdates(MBB, MBBI, StackPtr, true);
-      emitSPUpdate(MBB, MBBI, StackPtr, Offset, Is64Bit, UseLEA, TII, *RegInfo);
+      emitSPUpdate(MBB, MBBI, StackPtr, Offset, Is64Bit, STI.isLP64(), UseLEA, TII, *RegInfo);
     }
 
     // Jump to label or value in register.
-    if (RetOpcode == X86::TCRETURNdi || RetOpcode == X86::TCRETURNdi64) {
+    if (RetOpcode == X86::TCRETURNdi || RetOpcode == X86::TCRETURNdi64 ||
+        RetOpcode == X86::TCRETURNdiX32) {
       MachineInstrBuilder MIB =
         BuildMI(MBB, MBBI, DL, TII.get((RetOpcode == X86::TCRETURNdi)
-                                       ? X86::TAILJMPd : X86::TAILJMPd64));
+                                       ? X86::TAILJMPd
+                                       : (RetOpcode == X86::TCRETURNdiX32)
+                                         ? X86::TAILJMPdX32
+                                         : X86::TAILJMPd64));
       if (JumpTarget.isGlobal())
         MIB.addGlobalAddress(JumpTarget.getGlobal(), JumpTarget.getOffset(),
                              JumpTarget.getTargetFlags());
@@ -1118,6 +1146,9 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
     } else if (RetOpcode == X86::TCRETURNri64) {
       BuildMI(MBB, MBBI, DL, TII.get(X86::TAILJMPr64)).
         addReg(JumpTarget.getReg(), RegState::Kill);
+    } else if (RetOpcode == X86::TCRETURNriX32) {
+      BuildMI(MBB, MBBI, DL, TII.get(X86::TAILJMPrX32)).
+        addReg(JumpTarget.getReg(), RegState::Kill);
     } else {
       BuildMI(MBB, MBBI, DL, TII.get(X86::TAILJMPr)).
         addReg(JumpTarget.getReg(), RegState::Kill);
@@ -1137,7 +1168,7 @@ void X86FrameLowering::emitEpilogue(MachineFunction &MF,
 
     // Check for possible merge with preceding ADD instruction.
     delta += mergeSPUpdates(MBB, MBBI, StackPtr, true);
-    emitSPUpdate(MBB, MBBI, StackPtr, delta, Is64Bit, UseLEA, TII, *RegInfo);
+    emitSPUpdate(MBB, MBBI, StackPtr, delta, Is64Bit, STI.isLP64(), UseLEA, TII, *RegInfo);
   }
 }
 
@@ -1204,7 +1235,9 @@ bool X86FrameLowering::spillCalleeSavedRegisters(MachineBasicBlock &MBB,
   X86MachineFunctionInfo *X86FI = MF.getInfo<X86MachineFunctionInfo>();
 
   // Push GPRs. It increases frame size.
-  unsigned Opc = STI.is64Bit() ? X86::PUSH64r : X86::PUSH32r;
+  unsigned Opc = STI.is64Bit()
+                 ? (STI.isX32() ? X86::PUSHX32r : X86::PUSH64r)
+                 : X86::PUSH32r;
   for (unsigned i = CSI.size(); i != 0; --i) {
     unsigned Reg = CSI[i-1].getReg();
     if (!X86::GR64RegClass.contains(Reg) &&
@@ -1212,7 +1245,7 @@ bool X86FrameLowering::spillCalleeSavedRegisters(MachineBasicBlock &MBB,
       continue;
     // Add the callee-saved register as live-in. It's killed at the spill.
     MBB.addLiveIn(Reg);
-    if (Reg == FPReg)
+    if (TRI->regsOverlap(Reg, FPReg))
       // X86RegisterInfo::emitPrologue will handle spilling of frame register.
       continue;
     CalleeFrameSize += SlotSize;
@@ -1265,13 +1298,15 @@ bool X86FrameLowering::restoreCalleeSavedRegisters(MachineBasicBlock &MBB,
 
   // POP GPRs.
   unsigned FPReg = TRI->getFrameRegister(MF);
-  unsigned Opc = STI.is64Bit() ? X86::POP64r : X86::POP32r;
+  unsigned Opc = STI.is64Bit()
+                 ? (STI.isX32() ? X86::POPX32r : X86::POP64r)
+                 : X86::POP32r;
   for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
     unsigned Reg = CSI[i].getReg();
     if (!X86::GR64RegClass.contains(Reg) &&
         !X86::GR32RegClass.contains(Reg))
       continue;
-    if (Reg == FPReg)
+    if (TRI->regsOverlap(Reg, FPReg))
       // X86RegisterInfo::emitEpilogue will handle restoring of frame register.
       continue;
     BuildMI(MBB, MI, DL, TII.get(Opc), Reg);
@@ -1337,9 +1372,13 @@ HasNestArgument(const MachineFunction *MF) {
 /// either one or two registers will be needed. Set primary to true for
 /// the first register, false for the second.
 static unsigned
-GetScratchRegister(bool Is64Bit, const MachineFunction &MF, bool Primary) {
-  if (Is64Bit)
-    return Primary ? X86::R11 : X86::R12;
+GetScratchRegister(bool Is64Bit, bool IsX32, const MachineFunction &MF, bool Primary) {
+  if (Is64Bit) {
+    if (IsX32)
+      return Primary ? X86::R11D : X86::R12D;
+    else
+      return Primary ? X86::R11 : X86::R12;
+  }
 
   CallingConv::ID CallingConvention = MF.getFunction()->getCallingConv();
   bool IsNested = HasNestArgument(&MF);
@@ -1369,16 +1408,16 @@ X86FrameLowering::adjustForSegmentedStacks(MachineFunction &MF) const {
   bool Is64Bit = STI.is64Bit();
   unsigned TlsReg, TlsOffset;
   DebugLoc DL;
-  const X86Subtarget *ST = &MF.getTarget().getSubtarget<X86Subtarget>();
+//  const X86Subtarget *ST = &MF.getTarget().getSubtarget<X86Subtarget>();
 
-  unsigned ScratchReg = GetScratchRegister(Is64Bit, MF, true);
+  unsigned ScratchReg = GetScratchRegister(Is64Bit, STI.isX32(), MF, true);
   assert(!MF.getRegInfo().isLiveIn(ScratchReg) &&
          "Scratch register is live-in");
 
   if (MF.getFunction()->isVarArg())
     report_fatal_error("Segmented stacks do not support vararg functions.");
-  if (!ST->isTargetLinux() && !ST->isTargetDarwin() &&
-      !ST->isTargetWin32() && !ST->isTargetFreeBSD())
+  if (!STI.isTargetLinux() && !STI.isTargetDarwin() &&
+      !STI.isTargetWin32() && !STI.isTargetFreeBSD())
     report_fatal_error("Segmented stacks not supported on this platform.");
 
   MachineBasicBlock *allocMBB = MF.CreateMachineBasicBlock();
@@ -1415,14 +1454,14 @@ X86FrameLowering::adjustForSegmentedStacks(MachineFunction &MF) const {
   bool CompareStackPointer = StackSize < kSplitStackAvailable;
 
   // Read the limit off the current stacklet off the stack_guard location.
-  if (Is64Bit) {
-    if (ST->isTargetLinux()) {
+  if (STI.isLP64()) {
+    if (STI.isTargetLinux()) {
       TlsReg = X86::FS;
       TlsOffset = 0x70;
-    } else if (ST->isTargetDarwin()) {
+    } else if (STI.isTargetDarwin()) {
       TlsReg = X86::GS;
       TlsOffset = 0x60 + 90*8; // See pthread_machdep.h. Steal TLS slot 90.
-    } else if (ST->isTargetFreeBSD()) {
+    } else if (STI.isTargetFreeBSD()) {
       TlsReg = X86::FS;
       TlsOffset = 0x18;
     } else {
@@ -1437,17 +1476,33 @@ X86FrameLowering::adjustForSegmentedStacks(MachineFunction &MF) const {
 
     BuildMI(checkMBB, DL, TII.get(X86::CMP64rm)).addReg(ScratchReg)
       .addReg(0).addImm(1).addReg(0).addImm(TlsOffset).addReg(TlsReg);
+  } else if (STI.isX32()) {
+    if (STI.isTargetLinux()) {
+      TlsReg = X86::FS;
+      TlsOffset = 0x70;
+    } else {
+      report_fatal_error("Segmented stacks not supported on this platform.");
+    }
+
+    if (CompareStackPointer)
+      ScratchReg = X86::ESP;
+    else
+      BuildMI(checkMBB, DL, TII.get(X86::LEAX32r), ScratchReg).addReg(X86::ESP)
+        .addImm(1).addReg(0).addImm(-StackSize).addReg(0);
+
+    BuildMI(checkMBB, DL, TII.get(X86::CMP32rm)).addReg(ScratchReg)
+      .addReg(0).addImm(1).addReg(0).addImm(TlsOffset).addReg(TlsReg);
   } else {
-    if (ST->isTargetLinux()) {
+    if (STI.isTargetLinux()) {
       TlsReg = X86::GS;
       TlsOffset = 0x30;
-    } else if (ST->isTargetDarwin()) {
+    } else if (STI.isTargetDarwin()) {
       TlsReg = X86::GS;
       TlsOffset = 0x48 + 90*4;
-    } else if (ST->isTargetWin32()) {
+    } else if (STI.isTargetWin32()) {
       TlsReg = X86::FS;
       TlsOffset = 0x14; // pvArbitrary, reserved for application use
-    } else if (ST->isTargetFreeBSD()) {
+    } else if (STI.isTargetFreeBSD()) {
       report_fatal_error("Segmented stacks not supported on FreeBSD i386.");
     } else {
       report_fatal_error("Segmented stacks not supported on this platform.");
@@ -1459,21 +1514,21 @@ X86FrameLowering::adjustForSegmentedStacks(MachineFunction &MF) const {
       BuildMI(checkMBB, DL, TII.get(X86::LEA32r), ScratchReg).addReg(X86::ESP)
         .addImm(1).addReg(0).addImm(-StackSize).addReg(0);
 
-    if (ST->isTargetLinux() || ST->isTargetWin32()) {
+    if (STI.isTargetLinux() || STI.isTargetWin32()) {
       BuildMI(checkMBB, DL, TII.get(X86::CMP32rm)).addReg(ScratchReg)
         .addReg(0).addImm(0).addReg(0).addImm(TlsOffset).addReg(TlsReg);
-    } else if (ST->isTargetDarwin()) {
+    } else if (STI.isTargetDarwin()) {
 
       // TlsOffset doesn't fit into a mod r/m byte so we need an extra register
       unsigned ScratchReg2;
       bool SaveScratch2;
       if (CompareStackPointer) {
         // The primary scratch register is available for holding the TLS offset
-        ScratchReg2 = GetScratchRegister(Is64Bit, MF, true);
+        ScratchReg2 = GetScratchRegister(Is64Bit, STI.isX32(), MF, true);
         SaveScratch2 = false;
       } else {
         // Need to use a second register to hold the TLS offset
-        ScratchReg2 = GetScratchRegister(Is64Bit, MF, false);
+        ScratchReg2 = GetScratchRegister(Is64Bit, STI.isX32(), MF, false);
 
         // Unfortunately, with fastcc the second scratch register may hold an arg
         SaveScratch2 = MF.getRegInfo().isLiveIn(ScratchReg2);
@@ -1510,15 +1565,21 @@ X86FrameLowering::adjustForSegmentedStacks(MachineFunction &MF) const {
     // Functions with nested arguments use R10, so it needs to be saved across
     // the call to _morestack
 
+    unsigned RegAX = STI.isX32() ? X86::EAX : X86::RAX;
+    unsigned Reg10 = STI.isX32() ? X86::R10D : X86::R10;
+    unsigned Reg11 = STI.isX32() ? X86::R11D : X86::R11;
+    unsigned PMOVOpc = STI.isX32() ? X86::MOV32rr : X86::MOV64rr;
+    unsigned SMOViOpc = STI.isX32() ? X86::MOV32ri : X86::MOV64ri;
+
     if (IsNested)
-      BuildMI(allocMBB, DL, TII.get(X86::MOV64rr), X86::RAX).addReg(X86::R10);
+      BuildMI(allocMBB, DL, TII.get(PMOVOpc), RegAX).addReg(Reg10);
 
-    BuildMI(allocMBB, DL, TII.get(X86::MOV64ri), X86::R10)
+    BuildMI(allocMBB, DL, TII.get(SMOViOpc), Reg10)
       .addImm(StackSize);
-    BuildMI(allocMBB, DL, TII.get(X86::MOV64ri), X86::R11)
+    BuildMI(allocMBB, DL, TII.get(SMOViOpc), Reg11)
       .addImm(X86FI->getArgumentStackSize());
-    MF.getRegInfo().setPhysRegUsed(X86::R10);
-    MF.getRegInfo().setPhysRegUsed(X86::R11);
+    MF.getRegInfo().setPhysRegUsed(Reg10);
+    MF.getRegInfo().setPhysRegUsed(Reg11);
   } else {
     BuildMI(allocMBB, DL, TII.get(X86::PUSHi32))
       .addImm(X86FI->getArgumentStackSize());
@@ -1528,7 +1589,8 @@ X86FrameLowering::adjustForSegmentedStacks(MachineFunction &MF) const {
 
   // __morestack is in libgcc
   if (Is64Bit)
-    BuildMI(allocMBB, DL, TII.get(X86::CALL64pcrel32))
+    BuildMI(allocMBB, DL, TII.get(STI.isX32()
+                                  ? X86::CALLX32pcrel32 : X86::CALL64pcrel32))
       .addExternalSymbol("__morestack");
   else
     BuildMI(allocMBB, DL, TII.get(X86::CALLpcrel32))
diff --git a/lib/Target/X86/X86ISelLowering.cpp b/lib/Target/X86/X86ISelLowering.cpp
index 4baa1a6..b7d82128 100644
--- a/lib/Target/X86/X86ISelLowering.cpp
+++ b/lib/Target/X86/X86ISelLowering.cpp
@@ -152,7 +152,7 @@ X86TargetLowering::X86TargetLowering(X86TargetMachine &TM)
   Subtarget = &TM.getSubtarget<X86Subtarget>();
   X86ScalarSSEf64 = Subtarget->hasSSE2();
   X86ScalarSSEf32 = Subtarget->hasSSE1();
-  X86StackPtr = Subtarget->is64Bit() ? X86::RSP : X86::ESP;
+  X86StackPtr = Subtarget->isLP64() ? X86::RSP : X86::ESP;
 
   RegInfo = TM.getRegisterInfo();
   TD = getTargetData();
@@ -521,7 +521,7 @@ X86TargetLowering::X86TargetLowering(X86TargetMachine &TM)
   setOperationAction(ISD::EHSELECTION,   MVT::i64, Expand);
   setOperationAction(ISD::EXCEPTIONADDR, MVT::i32, Expand);
   setOperationAction(ISD::EHSELECTION,   MVT::i32, Expand);
-  if (Subtarget->is64Bit()) {
+  if (Subtarget->isLP64()) {
     setExceptionPointerRegister(X86::RAX);
     setExceptionSelectorRegister(X86::RDX);
   } else {
@@ -551,13 +551,13 @@ X86TargetLowering::X86TargetLowering(X86TargetMachine &TM)
   setOperationAction(ISD::STACKRESTORE,       MVT::Other, Expand);
 
   if (Subtarget->isTargetCOFF() && !Subtarget->isTargetEnvMacho())
-    setOperationAction(ISD::DYNAMIC_STACKALLOC, Subtarget->is64Bit() ?
+    setOperationAction(ISD::DYNAMIC_STACKALLOC, Subtarget->isLP64() ?
                        MVT::i64 : MVT::i32, Custom);
   else if (TM.Options.EnableSegmentedStacks)
-    setOperationAction(ISD::DYNAMIC_STACKALLOC, Subtarget->is64Bit() ?
+    setOperationAction(ISD::DYNAMIC_STACKALLOC, Subtarget->isLP64() ?
                        MVT::i64 : MVT::i32, Custom);
   else
-    setOperationAction(ISD::DYNAMIC_STACKALLOC, Subtarget->is64Bit() ?
+    setOperationAction(ISD::DYNAMIC_STACKALLOC, Subtarget->isLP64() ?
                        MVT::i64 : MVT::i32, Expand);
 
   if (!TM.Options.UseSoftFloat && X86ScalarSSEf64) {
@@ -1574,11 +1574,13 @@ X86TargetLowering::LowerReturn(SDValue Chain,
            "SRetReturnReg should have been set in LowerFormalArguments().");
     SDValue Val = DAG.getCopyFromReg(Chain, dl, Reg, getPointerTy());
 
-    Chain = DAG.getCopyToReg(Chain, dl, X86::RAX, Val, Flag);
+    unsigned RetValReg = Subtarget->isX32() ? X86::EAX : X86::RAX;
+
+    Chain = DAG.getCopyToReg(Chain, dl, RetValReg, Val, Flag);
     Flag = Chain.getValue(1);
 
-    // RAX now acts like a return value.
-    MRI.addLiveOut(X86::RAX);
+    // RAX/EAX now acts like a return value.
+    MRI.addLiveOut(RetValReg);
   }
 
   RetOps[0] = Chain;  // Update chain.
@@ -1922,7 +1924,8 @@ X86TargetLowering::LowerFormalArguments(SDValue Chain,
     X86MachineFunctionInfo *FuncInfo = MF.getInfo<X86MachineFunctionInfo>();
     unsigned Reg = FuncInfo->getSRetReturnReg();
     if (!Reg) {
-      Reg = MF.getRegInfo().createVirtualRegister(getRegClassFor(MVT::i64));
+      MVT PtrTy = Subtarget->isX32() ? MVT::i32 : MVT::i64;
+      Reg = MF.getRegInfo().createVirtualRegister(getRegClassFor(PtrTy));
       FuncInfo->setSRetReturnReg(Reg);
     }
     SDValue Copy = DAG.getCopyToReg(DAG.getEntryNode(), dl, Reg, InVals[0]);
@@ -2670,7 +2673,8 @@ bool MatchingStackOffset(SDValue Arg, unsigned Offset, ISD::ArgFlagsTy Flags,
         return false;
     } else {
       unsigned Opcode = Def->getOpcode();
-      if ((Opcode == X86::LEA32r || Opcode == X86::LEA64r) &&
+      if ((Opcode == X86::LEA32r || Opcode == X86::LEA64r ||
+           Opcode == X86::LEAX32r) &&
           Def->getOperand(1).isFI()) {
         FI = Def->getOperand(1).getIndex();
         Bytes = Flags.getByValSize();
@@ -2996,7 +3000,7 @@ SDValue X86TargetLowering::getReturnAddressFrameIndex(SelectionDAG &DAG) const {
 
   if (ReturnAddrIndex == 0) {
     // Set up a frame object for the return address.
-    uint64_t SlotSize = TD->getPointerSize();
+    uint64_t SlotSize = Subtarget->isX32() ? 8 : TD->getPointerSize();
     ReturnAddrIndex = MF.getFrameInfo()->CreateFixedObject(SlotSize, -SlotSize,
                                                            false);
     FuncInfo->setRAIndex(ReturnAddrIndex);
@@ -7307,15 +7311,16 @@ LowerToTLSGeneralDynamicModel32(GlobalAddressSDNode *GA, SelectionDAG &DAG,
 // Lower ISD::GlobalTLSAddress using the "general dynamic" model, 64 bit
 static SDValue
 LowerToTLSGeneralDynamicModel64(GlobalAddressSDNode *GA, SelectionDAG &DAG,
-                                const EVT PtrVT) {
+                                const EVT PtrVT, bool IsX32) {
   return GetTLSADDR(DAG, DAG.getEntryNode(), GA, NULL, PtrVT,
-                    X86::RAX, X86II::MO_TLSGD);
+                    IsX32? X86::EAX : X86::RAX, X86II::MO_TLSGD);
 }
 
 static SDValue LowerToTLSLocalDynamicModel(GlobalAddressSDNode *GA,
                                            SelectionDAG &DAG,
                                            const EVT PtrVT,
-                                           bool is64Bit) {
+                                           bool Is64Bit,
+                                           bool IsX32) {
   DebugLoc dl = GA->getDebugLoc();
 
   // Get the start address of the TLS block for this module.
@@ -7324,8 +7329,9 @@ static SDValue LowerToTLSLocalDynamicModel(GlobalAddressSDNode *GA,
   MFI->incNumLocalDynamicTLSAccesses();
 
   SDValue Base;
-  if (is64Bit) {
-    Base = GetTLSADDR(DAG, DAG.getEntryNode(), GA, NULL, PtrVT, X86::RAX,
+  if (Is64Bit) {
+    Base = GetTLSADDR(DAG, DAG.getEntryNode(), GA, NULL, PtrVT,
+                      IsX32 ? X86::EAX : X86::RAX,
                       X86II::MO_TLSLD, /*LocalDynamic=*/true);
   } else {
     SDValue InFlag;
@@ -7425,11 +7431,13 @@ X86TargetLowering::LowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const {
     switch (model) {
       case TLSModel::GeneralDynamic:
         if (Subtarget->is64Bit())
-          return LowerToTLSGeneralDynamicModel64(GA, DAG, getPointerTy());
+          return LowerToTLSGeneralDynamicModel64(GA, DAG, getPointerTy(),
+                                                 Subtarget->isX32());
         return LowerToTLSGeneralDynamicModel32(GA, DAG, getPointerTy());
       case TLSModel::LocalDynamic:
         return LowerToTLSLocalDynamicModel(GA, DAG, getPointerTy(),
-                                           Subtarget->is64Bit());
+                                           Subtarget->is64Bit(),
+                                           Subtarget->isX32());
       case TLSModel::InitialExec:
       case TLSModel::LocalExec:
         return LowerToTLSExecModel(GA, DAG, getPointerTy(), model,
@@ -7479,7 +7487,7 @@ X86TargetLowering::LowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const {
 
     // And our return value (tls address) is in the standard call return value
     // location.
-    unsigned Reg = Subtarget->is64Bit() ? X86::RAX : X86::EAX;
+    unsigned Reg = Subtarget->isLP64() ? X86::RAX : X86::EAX;
     return DAG.getCopyFromReg(Chain, DL, Reg, getPointerTy(),
                               Chain.getValue(1));
   }
@@ -9191,7 +9199,7 @@ X86TargetLowering::LowerDYNAMIC_STACKALLOC(SDValue Op,
   // FIXME: Ensure alignment here
 
   bool Is64Bit = Subtarget->is64Bit();
-  EVT SPTy = Is64Bit ? MVT::i64 : MVT::i32;
+  EVT SPTy = Subtarget->isLP64() ? MVT::i64 : MVT::i32;
 
   if (getTargetMachine().Options.EnableSegmentedStacks) {
     MachineFunction &MF = DAG.getMachineFunction();
@@ -9210,7 +9218,7 @@ X86TargetLowering::LowerDYNAMIC_STACKALLOC(SDValue Op,
     }
 
     const TargetRegisterClass *AddrRegClass =
-      getRegClassFor(Subtarget->is64Bit() ? MVT::i64:MVT::i32);
+      getRegClassFor(Subtarget->isLP64() ? MVT::i64:MVT::i32);
     unsigned Vreg = MRI.createVirtualRegister(AddrRegClass);
     Chain = DAG.getCopyToReg(Chain, dl, Vreg, Size);
     SDValue Value = DAG.getNode(X86ISD::SEG_ALLOCA, dl, SPTy, Chain,
@@ -9219,7 +9227,7 @@ X86TargetLowering::LowerDYNAMIC_STACKALLOC(SDValue Op,
     return DAG.getMergeValues(Ops1, 2, dl);
   } else {
     SDValue Flag;
-    unsigned Reg = (Subtarget->is64Bit() ? X86::RAX : X86::EAX);
+    unsigned Reg = (Subtarget->isLP64() ? X86::RAX : X86::EAX);
 
     Chain = DAG.getCopyToReg(Chain, dl, Reg, Size, Flag);
     Flag = Chain.getValue(1);
@@ -9251,6 +9259,7 @@ SDValue X86TargetLowering::LowerVASTART(SDValue Op, SelectionDAG &DAG) const {
                         MachinePointerInfo(SV), false, false, 0);
   }
 
+  unsigned offset = 0, rel;
   // __va_list_tag:
   //   gp_offset         (0 - 6 * 8)
   //   fp_offset         (48 - 48 + 8 * 16)
@@ -9266,31 +9275,34 @@ SDValue X86TargetLowering::LowerVASTART(SDValue Op, SelectionDAG &DAG) const {
   MemOps.push_back(Store);
 
   // Store fp_offset
+  offset += (rel = 4);
   FIN = DAG.getNode(ISD::ADD, DL, getPointerTy(),
-                    FIN, DAG.getIntPtrConstant(4));
+                    FIN, DAG.getIntPtrConstant(rel));
   Store = DAG.getStore(Op.getOperand(0), DL,
                        DAG.getConstant(FuncInfo->getVarArgsFPOffset(),
                                        MVT::i32),
-                       FIN, MachinePointerInfo(SV, 4), false, false, 0);
+                       FIN, MachinePointerInfo(SV, offset), false, false, 0);
   MemOps.push_back(Store);
 
   // Store ptr to overflow_arg_area
+  offset += (rel = 4);
   FIN = DAG.getNode(ISD::ADD, DL, getPointerTy(),
-                    FIN, DAG.getIntPtrConstant(4));
+                    FIN, DAG.getIntPtrConstant(rel));
   SDValue OVFIN = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),
                                     getPointerTy());
   Store = DAG.getStore(Op.getOperand(0), DL, OVFIN, FIN,
-                       MachinePointerInfo(SV, 8),
+                       MachinePointerInfo(SV, offset),
                        false, false, 0);
   MemOps.push_back(Store);
 
   // Store ptr to reg_save_area.
+  offset += (rel = Subtarget->isX32() ? 4 : 8);
   FIN = DAG.getNode(ISD::ADD, DL, getPointerTy(),
-                    FIN, DAG.getIntPtrConstant(8));
+                    FIN, DAG.getIntPtrConstant(rel));
   SDValue RSFIN = DAG.getFrameIndex(FuncInfo->getRegSaveFrameIndex(),
                                     getPointerTy());
   Store = DAG.getStore(Op.getOperand(0), DL, RSFIN, FIN,
-                       MachinePointerInfo(SV, 16), false, false, 0);
+                       MachinePointerInfo(SV, offset), false, false, 0);
   MemOps.push_back(Store);
   return DAG.getNode(ISD::TokenFactor, DL, MVT::Other,
                      &MemOps[0], MemOps.size());
@@ -9373,8 +9385,8 @@ SDValue X86TargetLowering::LowerVACOPY(SDValue Op, SelectionDAG &DAG) const {
   DebugLoc DL = Op.getDebugLoc();
 
   return DAG.getMemcpy(Chain, DL, DstPtr, SrcPtr,
-                       DAG.getIntPtrConstant(24), 8, /*isVolatile*/false,
-                       false,
+                       DAG.getIntPtrConstant(Subtarget->isX32() ? 16 : 24), 8,
+                       /*isVolatile*/false, false,
                        MachinePointerInfo(DstSV), MachinePointerInfo(SrcSV));
 }
 
@@ -9965,8 +9977,8 @@ SDValue X86TargetLowering::LowerRETURNADDR(SDValue Op,
   if (Depth > 0) {
     SDValue FrameAddr = LowerFRAMEADDR(Op, DAG);
     SDValue Offset =
-      DAG.getConstant(TD->getPointerSize(),
-                      Subtarget->is64Bit() ? MVT::i64 : MVT::i32);
+      DAG.getConstant(Subtarget->isX32() ? 8 : TD->getPointerSize(),
+                      Subtarget->isLP64() ? MVT::i64 : MVT::i32);
     return DAG.getLoad(getPointerTy(), dl, DAG.getEntryNode(),
                        DAG.getNode(ISD::ADD, dl, getPointerTy(),
                                    FrameAddr, Offset),
@@ -9986,7 +9998,7 @@ SDValue X86TargetLowering::LowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const {
   EVT VT = Op.getValueType();
   DebugLoc dl = Op.getDebugLoc();  // FIXME probably not meaningful
   unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue();
-  unsigned FrameReg = Subtarget->is64Bit() ? X86::RBP : X86::EBP;
+  unsigned FrameReg = Subtarget->isLP64() ? X86::RBP : X86::EBP;
   SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), dl, FrameReg, VT);
   while (Depth--)
     FrameAddr = DAG.getLoad(VT, dl, DAG.getEntryNode(), FrameAddr,
@@ -10008,9 +10020,9 @@ SDValue X86TargetLowering::LowerEH_RETURN(SDValue Op, SelectionDAG &DAG) const {
   DebugLoc dl       = Op.getDebugLoc();
 
   SDValue Frame = DAG.getCopyFromReg(DAG.getEntryNode(), dl,
-                                     Subtarget->is64Bit() ? X86::RBP : X86::EBP,
+                                     Subtarget->isLP64() ? X86::RBP : X86::EBP,
                                      getPointerTy());
-  unsigned StoreAddrReg = (Subtarget->is64Bit() ? X86::RCX : X86::ECX);
+  unsigned StoreAddrReg = (Subtarget->isLP64() ? X86::RCX : X86::ECX);
 
   SDValue StoreAddr = DAG.getNode(ISD::ADD, dl, getPointerTy(), Frame,
                                   DAG.getIntPtrConstant(TD->getPointerSize()));
@@ -10045,54 +10057,64 @@ SDValue X86TargetLowering::LowerINIT_TRAMPOLINE(SDValue Op,
 
     // Large code-model.
     const unsigned char JMP64r  = 0xFF; // 64-bit jmp through register opcode.
-    const unsigned char MOV64ri = 0xB8; // X86::MOV64ri opcode.
+    const unsigned char MOVri = 0xB8; // X86::MOV{32|64}ri opcode.
 
     const unsigned char N86R10 = X86_MC::getX86RegNum(X86::R10);
     const unsigned char N86R11 = X86_MC::getX86RegNum(X86::R11);
 
-    const unsigned char REX_WB = 0x40 | 0x08 | 0x01; // REX prefix
+    // REX prefix with REX.B and/or REX.W
+    const unsigned char REX_wB = 0x40 | 0x01 |
+                                 (Subtarget->isX32() ? 0 : 0x08);
+
+    EVT PtrTy = getPointerTy();
+    unsigned offset = 0;
 
     // Load the pointer to the nested function into R11.
-    unsigned OpCode = ((MOV64ri | N86R11) << 8) | REX_WB; // movabsq r11
+    unsigned OpCode = ((MOVri | N86R11) << 8) | REX_wB; // movabs{q} r11{d}
     SDValue Addr = Trmp;
     OutChains[0] = DAG.getStore(Root, dl, DAG.getConstant(OpCode, MVT::i16),
                                 Addr, MachinePointerInfo(TrmpAddr),
                                 false, false, 0);
+    offset += 2;
 
-    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,
-                       DAG.getConstant(2, MVT::i64));
+    Addr = DAG.getNode(ISD::ADD, dl, PtrTy, Trmp,
+                       DAG.getConstant(2, PtrTy));
     OutChains[1] = DAG.getStore(Root, dl, FPtr, Addr,
                                 MachinePointerInfo(TrmpAddr, 2),
                                 false, false, 2);
+    offset += PtrTy.getStoreSize();
 
     // Load the 'nest' parameter value into R10.
     // R10 is specified in X86CallingConv.td
-    OpCode = ((MOV64ri | N86R10) << 8) | REX_WB; // movabsq r10
-    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,
-                       DAG.getConstant(10, MVT::i64));
+    OpCode = ((MOVri | N86R10) << 8) | REX_wB; // movabs{q} r10{d}
+    Addr = DAG.getNode(ISD::ADD, dl, PtrTy, Trmp,
+                       DAG.getConstant(offset, PtrTy));
     OutChains[2] = DAG.getStore(Root, dl, DAG.getConstant(OpCode, MVT::i16),
-                                Addr, MachinePointerInfo(TrmpAddr, 10),
+                                Addr, MachinePointerInfo(TrmpAddr, offset),
                                 false, false, 0);
+    offset += 2;
 
-    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,
-                       DAG.getConstant(12, MVT::i64));
+    Addr = DAG.getNode(ISD::ADD, dl, PtrTy, Trmp,
+                       DAG.getConstant(offset, PtrTy));
     OutChains[3] = DAG.getStore(Root, dl, Nest, Addr,
-                                MachinePointerInfo(TrmpAddr, 12),
+                                MachinePointerInfo(TrmpAddr, offset),
                                 false, false, 2);
+    offset += PtrTy.getStoreSize();
 
     // Jump to the nested function.
-    OpCode = (JMP64r << 8) | REX_WB; // jmpq *...
-    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,
-                       DAG.getConstant(20, MVT::i64));
+    OpCode = (JMP64r << 8) | REX_wB; // jmpq *...
+    Addr = DAG.getNode(ISD::ADD, dl, PtrTy, Trmp,
+                       DAG.getConstant(offset, PtrTy));
     OutChains[4] = DAG.getStore(Root, dl, DAG.getConstant(OpCode, MVT::i16),
-                                Addr, MachinePointerInfo(TrmpAddr, 20),
+                                Addr, MachinePointerInfo(TrmpAddr, offset),
                                 false, false, 0);
+    offset += 2;
 
     unsigned char ModRM = N86R11 | (4 << 3) | (3 << 6); // ...r11
-    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,
-                       DAG.getConstant(22, MVT::i64));
+    Addr = DAG.getNode(ISD::ADD, dl, PtrTy, Trmp,
+                       DAG.getConstant(offset, PtrTy));
     OutChains[5] = DAG.getStore(Root, dl, DAG.getConstant(ModRM, MVT::i8), Addr,
-                                MachinePointerInfo(TrmpAddr, 22),
+                                MachinePointerInfo(TrmpAddr, offset),
                                 false, false, 0);
 
     return DAG.getNode(ISD::TokenFactor, dl, MVT::Other, OutChains, 6);
@@ -11940,8 +11962,10 @@ X86TargetLowering::EmitMonitor(MachineInstr *MI, MachineBasicBlock *BB) const {
   const TargetInstrInfo *TII = getTargetMachine().getInstrInfo();
 
   // Address into RAX/EAX, other two args into ECX, EDX.
-  unsigned MemOpc = Subtarget->is64Bit() ? X86::LEA64r : X86::LEA32r;
-  unsigned MemReg = Subtarget->is64Bit() ? X86::RAX : X86::EAX;
+  unsigned MemOpc = Subtarget->is64Bit()
+                    ? (Subtarget->isX32() ? X86::LEAX32r : X86::LEA64r)
+                    : X86::LEA32r;
+  unsigned MemReg = Subtarget->isLP64() ? X86::RAX : X86::EAX;
   MachineInstrBuilder MIB = BuildMI(*BB, MI, dl, TII->get(MemOpc), MemReg);
   for (int i = 0; i < X86::AddrNumOperands; ++i)
     MIB.addOperand(MI->getOperand(i));
@@ -12012,7 +12036,9 @@ X86TargetLowering::EmitVAARG64WithCustomInserter(
   // Machine Information
   const TargetInstrInfo *TII = getTargetMachine().getInstrInfo();
   MachineRegisterInfo &MRI = MBB->getParent()->getRegInfo();
-  const TargetRegisterClass *AddrRegClass = getRegClassFor(MVT::i64);
+  const TargetRegisterClass *AddrRegClass = getRegClassFor(Subtarget->isX32()
+                                                           ? MVT::i32
+                                                           : MVT::i64);
   const TargetRegisterClass *OffsetRegClass = getRegClassFor(MVT::i32);
   DebugLoc DL = MI->getDebugLoc();
 
@@ -12126,25 +12152,33 @@ X86TargetLowering::EmitVAARG64WithCustomInserter(
 
     // Read the reg_save_area address.
     unsigned RegSaveReg = MRI.createVirtualRegister(AddrRegClass);
-    BuildMI(offsetMBB, DL, TII->get(X86::MOV64rm), RegSaveReg)
+    BuildMI(offsetMBB, DL, TII->get(Subtarget->isX32()
+                                    ? X86::MOV32rm : X86::MOV64rm), RegSaveReg)
       .addOperand(Base)
       .addOperand(Scale)
       .addOperand(Index)
-      .addDisp(Disp, 16)
+      .addDisp(Disp, Subtarget->isX32() ? 12 : 16)
       .addOperand(Segment)
       .setMemRefs(MMOBegin, MMOEnd);
 
-    // Zero-extend the offset
-    unsigned OffsetReg64 = MRI.createVirtualRegister(AddrRegClass);
-      BuildMI(offsetMBB, DL, TII->get(X86::SUBREG_TO_REG), OffsetReg64)
-        .addImm(0)
+    if (Subtarget->isLP64()) {
+      // Zero-extend the offset
+      unsigned OffsetReg64 = MRI.createVirtualRegister(AddrRegClass);
+        BuildMI(offsetMBB, DL, TII->get(X86::SUBREG_TO_REG), OffsetReg64)
+          .addImm(0)
+          .addReg(OffsetReg)
+          .addImm(X86::sub_32bit);
+
+      // Add the offset to the reg_save_area to get the final address.
+      BuildMI(offsetMBB, DL, TII->get(X86::ADD64rr), OffsetDestReg)
+        .addReg(OffsetReg64)
+        .addReg(RegSaveReg);
+    } else {
+      // Add the offset to the reg_save_area to get the final address.
+      BuildMI(offsetMBB, DL, TII->get(X86::ADD32rr), OffsetDestReg)
         .addReg(OffsetReg)
-        .addImm(X86::sub_32bit);
-
-    // Add the offset to the reg_save_area to get the final address.
-    BuildMI(offsetMBB, DL, TII->get(X86::ADD64rr), OffsetDestReg)
-      .addReg(OffsetReg64)
-      .addReg(RegSaveReg);
+        .addReg(RegSaveReg);
+    }
 
     // Compute the offset for the next argument
     unsigned NextOffsetReg = MRI.createVirtualRegister(OffsetRegClass);
@@ -12173,7 +12207,9 @@ X86TargetLowering::EmitVAARG64WithCustomInserter(
 
   // Load the overflow_area address into a register.
   unsigned OverflowAddrReg = MRI.createVirtualRegister(AddrRegClass);
-  BuildMI(overflowMBB, DL, TII->get(X86::MOV64rm), OverflowAddrReg)
+  BuildMI(overflowMBB, DL, TII->get(Subtarget->isX32()
+                                    ? X86::MOV32rm
+                                    : X86::MOV64rm), OverflowAddrReg)
     .addOperand(Base)
     .addOperand(Scale)
     .addOperand(Index)
@@ -12189,11 +12225,15 @@ X86TargetLowering::EmitVAARG64WithCustomInserter(
     unsigned TmpReg = MRI.createVirtualRegister(AddrRegClass);
 
     // aligned_addr = (addr + (align-1)) & ~(align-1)
-    BuildMI(overflowMBB, DL, TII->get(X86::ADD64ri32), TmpReg)
+    BuildMI(overflowMBB, DL, TII->get(Subtarget->isX32()
+                                      ? X86::ADD32ri
+                                      : X86::ADD64ri32), TmpReg)
       .addReg(OverflowAddrReg)
       .addImm(Align-1);
 
-    BuildMI(overflowMBB, DL, TII->get(X86::AND64ri32), OverflowDestReg)
+    BuildMI(overflowMBB, DL, TII->get(Subtarget->isX32()
+                                      ? X86::AND32ri
+                                      : X86::AND64ri32), OverflowDestReg)
       .addReg(TmpReg)
       .addImm(~(uint64_t)(Align-1));
   } else {
@@ -12204,12 +12244,15 @@ X86TargetLowering::EmitVAARG64WithCustomInserter(
   // Compute the next overflow address after this argument.
   // (the overflow address should be kept 8-byte aligned)
   unsigned NextAddrReg = MRI.createVirtualRegister(AddrRegClass);
-  BuildMI(overflowMBB, DL, TII->get(X86::ADD64ri32), NextAddrReg)
+  BuildMI(overflowMBB, DL, TII->get(Subtarget->isX32()
+                                    ? X86::ADD32ri
+                                    : X86::ADD64ri32), NextAddrReg)
     .addReg(OverflowDestReg)
     .addImm(ArgSizeA8);
 
   // Store the new overflow address.
-  BuildMI(overflowMBB, DL, TII->get(X86::MOV64mr))
+  BuildMI(overflowMBB, DL, TII->get(Subtarget->isX32()
+                                    ? X86::MOV32mr : X86::MOV64mr))
     .addOperand(Base)
     .addOperand(Scale)
     .addOperand(Index)
@@ -12445,14 +12488,14 @@ X86TargetLowering::EmitLoweredSegAlloca(MachineInstr *MI, MachineBasicBlock *BB,
 
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const TargetRegisterClass *AddrRegClass =
-    getRegClassFor(Is64Bit ? MVT::i64:MVT::i32);
+    getRegClassFor(Subtarget->isLP64() ? MVT::i64:MVT::i32);
 
   unsigned mallocPtrVReg = MRI.createVirtualRegister(AddrRegClass),
     bumpSPPtrVReg = MRI.createVirtualRegister(AddrRegClass),
     tmpSPVReg = MRI.createVirtualRegister(AddrRegClass),
     SPLimitVReg = MRI.createVirtualRegister(AddrRegClass),
     sizeVReg = MI->getOperand(1).getReg(),
-    physSPReg = Is64Bit ? X86::RSP : X86::ESP;
+    physSPReg = Subtarget->isLP64() ? X86::RSP : X86::ESP;
 
   MachineFunction::iterator MBBIter = BB;
   ++MBBIter;
@@ -12468,9 +12511,9 @@ X86TargetLowering::EmitLoweredSegAlloca(MachineInstr *MI, MachineBasicBlock *BB,
   // Add code to the main basic block to check if the stack limit has been hit,
   // and if so, jump to mallocMBB otherwise to bumpMBB.
   BuildMI(BB, DL, TII->get(TargetOpcode::COPY), tmpSPVReg).addReg(physSPReg);
-  BuildMI(BB, DL, TII->get(Is64Bit ? X86::SUB64rr:X86::SUB32rr), SPLimitVReg)
+  BuildMI(BB, DL, TII->get(Subtarget->isLP64() ? X86::SUB64rr:X86::SUB32rr), SPLimitVReg)
     .addReg(tmpSPVReg).addReg(sizeVReg);
-  BuildMI(BB, DL, TII->get(Is64Bit ? X86::CMP64mr:X86::CMP32mr))
+  BuildMI(BB, DL, TII->get(Subtarget->isLP64() ? X86::CMP64mr:X86::CMP32mr))
     .addReg(0).addImm(1).addReg(0).addImm(TlsOffset).addReg(TlsReg)
     .addReg(SPLimitVReg);
   BuildMI(BB, DL, TII->get(X86::JG_4)).addMBB(mallocMBB);
@@ -12487,12 +12530,21 @@ X86TargetLowering::EmitLoweredSegAlloca(MachineInstr *MI, MachineBasicBlock *BB,
   const uint32_t *RegMask =
     getTargetMachine().getRegisterInfo()->getCallPreservedMask(CallingConv::C);
   if (Is64Bit) {
-    BuildMI(mallocMBB, DL, TII->get(X86::MOV64rr), X86::RDI)
-      .addReg(sizeVReg);
-    BuildMI(mallocMBB, DL, TII->get(X86::CALL64pcrel32))
-      .addExternalSymbol("__morestack_allocate_stack_space").addReg(X86::RDI)
-      .addRegMask(RegMask)
-      .addReg(X86::RAX, RegState::ImplicitDefine);
+    if (Subtarget->isLP64()) {
+      BuildMI(mallocMBB, DL, TII->get(X86::MOV64rr), X86::RDI)
+        .addReg(sizeVReg);
+      BuildMI(mallocMBB, DL, TII->get(X86::CALL64pcrel32))
+        .addExternalSymbol("__morestack_allocate_stack_space").addReg(X86::RDI)
+        .addRegMask(RegMask)
+        .addReg(X86::RAX, RegState::ImplicitDefine);
+    } else {
+      BuildMI(mallocMBB, DL, TII->get(X86::MOV32rr), X86::EDI)
+        .addReg(sizeVReg);
+      BuildMI(mallocMBB, DL, TII->get(X86::CALLX32pcrel32))
+        .addExternalSymbol("__morestack_allocate_stack_space").addReg(X86::EDI)
+        .addRegMask(RegMask)
+        .addReg(X86::EAX, RegState::ImplicitDefine);
+    }
   } else {
     BuildMI(mallocMBB, DL, TII->get(X86::SUB32ri), physSPReg).addReg(physSPReg)
       .addImm(12);
@@ -12508,7 +12560,7 @@ X86TargetLowering::EmitLoweredSegAlloca(MachineInstr *MI, MachineBasicBlock *BB,
       .addImm(16);
 
   BuildMI(mallocMBB, DL, TII->get(TargetOpcode::COPY), mallocPtrVReg)
-    .addReg(Is64Bit ? X86::RAX : X86::EAX);
+    .addReg(Subtarget->isLP64() ? X86::RAX : X86::EAX);
   BuildMI(mallocMBB, DL, TII->get(X86::JMP_4)).addMBB(continueMBB);
 
   // Set up the CFG correctly.
@@ -12584,6 +12636,28 @@ X86TargetLowering::EmitLoweredWinAlloca(MachineInstr *MI,
 }
 
 MachineBasicBlock *
+X86TargetLowering::EmitCallX32m(MachineInstr *MI,
+                                MachineBasicBlock *MBB) const {
+  assert(Subtarget->isX32() && "Only valid in X32!");
+
+  const TargetInstrInfo *TII = getTargetMachine().getInstrInfo();
+  DebugLoc DL = MI->getDebugLoc();
+  MachineFunction *F = MBB->getParent();
+
+  unsigned TmpReg = F->getRegInfo().createVirtualRegister(&X86::GR32RegClass);
+
+  MachineInstrBuilder MIB = BuildMI(*MBB, MI, DL, TII->get(X86::MOV32rm), TmpReg);
+  for (unsigned i = 0; i < X86::AddrNumOperands; ++i)
+    MIB.addOperand(MI->getOperand(i));
+  MIB = BuildMI(*MBB, MI, DL, TII->get(X86::CALLX32r)).addReg(TmpReg);
+  for (unsigned i = X86::AddrNumOperands; i < MI->getNumExplicitOperands(); ++i)
+    MIB.addOperand(MI->getOperand(i));
+
+  MI->eraseFromParent(); // The pseudo instruction is gone now.
+  return MBB;
+}
+
+MachineBasicBlock *
 X86TargetLowering::EmitLoweredTLSCall(MachineInstr *MI,
                                       MachineBasicBlock *BB) const {
   // This is pretty easy.  We're taking the value that we received from
@@ -12647,10 +12721,16 @@ X86TargetLowering::EmitInstrWithCustomInserter(MachineInstr *MI,
                                                MachineBasicBlock *BB) const {
   switch (MI->getOpcode()) {
   default: llvm_unreachable("Unexpected instr type to insert");
+  case X86::CALLX32m:
+    return EmitCallX32m(MI, BB);
+  case X86::TAILJMPdX32:
+  case X86::TAILJMPrX32:
   case X86::TAILJMPd64:
   case X86::TAILJMPr64:
   case X86::TAILJMPm64:
     llvm_unreachable("TAILJMP64 would not be touched here.");
+  case X86::TCRETURNdiX32:
+  case X86::TCRETURNriX32:
   case X86::TCRETURNdi64:
   case X86::TCRETURNri64:
   case X86::TCRETURNmi64:
diff --git a/lib/Target/X86/X86ISelLowering.h b/lib/Target/X86/X86ISelLowering.h
index 6d6ff60..d6cbf87 100644
--- a/lib/Target/X86/X86ISelLowering.h
+++ b/lib/Target/X86/X86ISelLowering.h
@@ -903,6 +903,9 @@ namespace llvm {
     MachineBasicBlock *EmitLoweredTLSCall(MachineInstr *MI,
                                           MachineBasicBlock *BB) const;
 
+    MachineBasicBlock *EmitCallX32m(MachineInstr *MI,
+                                    MachineBasicBlock *BB) const;
+
     MachineBasicBlock *emitLoweredTLSAddr(MachineInstr *MI,
                                           MachineBasicBlock *BB) const;
 
diff --git a/lib/Target/X86/X86InstrArithmetic.td b/lib/Target/X86/X86InstrArithmetic.td
index 0eee083..d5a5e57 100644
--- a/lib/Target/X86/X86InstrArithmetic.td
+++ b/lib/Target/X86/X86InstrArithmetic.td
@@ -30,12 +30,28 @@ def LEA64_32r : I<0x8D, MRMSrcMem,
                   (outs GR32:$dst), (ins lea64_32mem:$src),
                   "lea{l}\t{$src|$dst}, {$dst|$src}",
                   [(set GR32:$dst, lea32addr:$src)], IIC_LEA>,
-                  Requires<[In64BitMode]>;
+                  Requires<[In64BitMode, IsLP64]>;
 
 let isReMaterializable = 1 in
 def LEA64r   : RI<0x8D, MRMSrcMem, (outs GR64:$dst), (ins i64mem:$src),
                   "lea{q}\t{$src|$dst}, {$dst|$src}",
-                  [(set GR64:$dst, lea64addr:$src)], IIC_LEA>;
+                  [(set GR64:$dst, lea64addr:$src)], IIC_LEA>,
+                  Requires<[IsLP64]>;
+
+let isCodeGenOnly = 1 in {
+  def LEAX32_64r : I<0x8D, MRMSrcMem,
+                     (outs GR64:$dst), (ins leax32_64mem:$src),
+                     "lea{q}\t{$src|$dst}, {$dst|$src}",
+                     [(set GR64:$dst, lea64addr:$src)], IIC_LEA>,
+                   Requires<[IsX32]>;
+
+  let isReMaterializable = 1 in
+    def LEAX32r  : I<0x8D, MRMSrcMem,
+                    (outs GR32:$dst), (ins i32mem:$src),
+                    "lea{l}\t{$src|$dst}, {$dst|$src}",
+                    [(set GR32:$dst, leax32addr:$src)], IIC_LEA>,
+                   Requires<[IsX32]>;
+}
 
 
 
diff --git a/lib/Target/X86/X86InstrCompiler.td b/lib/Target/X86/X86InstrCompiler.td
index 99c2b8f..e17efac 100644
--- a/lib/Target/X86/X86InstrCompiler.td
+++ b/lib/Target/X86/X86InstrCompiler.td
@@ -62,11 +62,27 @@ let Defs = [RSP, EFLAGS], Uses = [RSP] in {
 def ADJCALLSTACKDOWN64 : I<0, Pseudo, (outs), (ins i32imm:$amt),
                            "#ADJCALLSTACKDOWN",
                            [(X86callseq_start timm:$amt)]>,
-                          Requires<[In64BitMode]>;
+                          Requires<[IsLP64]>;
 def ADJCALLSTACKUP64   : I<0, Pseudo, (outs), (ins i32imm:$amt1, i32imm:$amt2),
                            "#ADJCALLSTACKUP",
                            [(X86callseq_end timm:$amt1, timm:$amt2)]>,
-                          Requires<[In64BitMode]>;
+                          Requires<[IsLP64]>;
+}
+
+// ADJCALLSTACKDOWN/UP implicitly use/def RSP because they may be expanded into
+// a stack adjustment and the codegen must know that they may modify the stack
+// pointer before prolog-epilog rewriting occurs.
+// Pessimistically assume ADJCALLSTACKDOWN / ADJCALLSTACKUP will become
+// sub / add which can clobber EFLAGS.
+let Defs = [ESP, EFLAGS], Uses = [ESP] in {
+def ADJCALLSTACKDOWNX32 : I<0, Pseudo, (outs), (ins i32imm:$amt),
+                            "#ADJCALLSTACKDOWN",
+                            [(X86callseq_start timm:$amt)]>,
+                          Requires<[IsX32]>;
+def ADJCALLSTACKUPX32   : I<0, Pseudo, (outs), (ins i32imm:$amt1, i32imm:$amt2),
+                            "#ADJCALLSTACKUP",
+                            [(X86callseq_end timm:$amt1, timm:$amt2)]>,
+                          Requires<[IsX32]>;
 }
 
 
@@ -124,7 +140,14 @@ def SEG_ALLOCA_64 : I<0, Pseudo, (outs GR64:$dst), (ins GR64:$size),
                       "# variable sized alloca for segmented stacks",
                       [(set GR64:$dst,
                          (X86SegAlloca GR64:$size))]>,
-                    Requires<[In64BitMode]>;
+                    Requires<[IsLP64]>;
+
+let Defs = [EAX, ESP, EFLAGS], Uses = [ESP] in
+def SEG_ALLOCA_X32: I<0, Pseudo, (outs GR32:$dst), (ins GR32:$size),
+                      "# variable sized alloca for segmented stacks",
+                      [(set GR32:$dst,
+                         (X86SegAlloca GR32:$size))]>,
+                    Requires<[IsX32]>;
 }
 
 // The MSVC runtime contains an _ftol2 routine for converting floating-point
@@ -153,16 +176,18 @@ let isTerminator = 1, isReturn = 1, isBarrier = 1,
     hasCtrlDep = 1, isCodeGenOnly = 1 in {
 def EH_RETURN   : I<0xC3, RawFrm, (outs), (ins GR32:$addr),
                     "ret\t#eh_return, addr: $addr",
-                    [(X86ehret GR32:$addr)], IIC_RET>;
-
-}
+                    [(X86ehret GR32:$addr)], IIC_RET>,
+                  Requires<[In32BitMode]>;
 
-let isTerminator = 1, isReturn = 1, isBarrier = 1,
-    hasCtrlDep = 1, isCodeGenOnly = 1 in {
 def EH_RETURN64   : I<0xC3, RawFrm, (outs), (ins GR64:$addr),
                      "ret\t#eh_return, addr: $addr",
-                     [(X86ehret GR64:$addr)], IIC_RET>;
+                     [(X86ehret GR64:$addr)], IIC_RET>,
+                    Requires<[IsLP64]>;
 
+def EH_RETURNX32  : I<0xC3, RawFrm, (outs), (ins GR32:$addr),
+                     "ret\t#eh_return, addr: $addr",
+                     [(X86ehret GR32:$addr)], IIC_RET>,
+                    Requires<[IsX32]>;
 }
 
 //===----------------------------------------------------------------------===//
@@ -398,11 +423,30 @@ let Defs = [RAX, RCX, RDX, RSI, RDI, R8, R9, R10, R11,
 def TLS_addr64 : I<0, Pseudo, (outs), (ins i64mem:$sym),
                    "# TLS_addr64",
                   [(X86tlsaddr tls64addr:$sym)]>,
-                  Requires<[In64BitMode]>;
+                  Requires<[IsLP64]>;
 def TLS_base_addr64 : I<0, Pseudo, (outs), (ins i64mem:$sym),
                    "# TLS_base_addr64",
                   [(X86tlsbaseaddr tls64baseaddr:$sym)]>,
-                  Requires<[In64BitMode]>;
+                  Requires<[IsLP64]>;
+}
+
+// All calls clobber the non-callee saved registers. ESP is marked as
+// a use to prevent stack-pointer assignments that appear immediately
+// before calls from potentially appearing dead.
+let Defs = [EAX, ECX, EDX, ESI, EDI, R8D, R9D, R10D, R11D,
+            FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0, ST1,
+            MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7,
+            XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7,
+            XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15, EFLAGS],
+    Uses = [ESP] in {
+def TLS_addrX32: I<0, Pseudo, (outs), (ins i32mem:$sym),
+                   "# TLS_addrx32",
+                  [(X86tlsaddr tls32addr:$sym)]>,
+                  Requires<[IsX32]>;
+def TLS_base_addrX32: I<0, Pseudo, (outs), (ins i32mem:$sym),
+                   "# TLS_base_addrx32",
+                  [(X86tlsbaseaddr tls32baseaddr:$sym)]>,
+                  Requires<[IsX32]>;
 }
 
 // Darwin TLS Support
@@ -425,7 +469,15 @@ let Defs = [RAX, EFLAGS],
 def TLSCall_64 : I<0, Pseudo, (outs), (ins i64mem:$sym),
                   "# TLSCall_64",
                   [(X86TLSCall addr:$sym)]>,
-                  Requires<[In64BitMode]>;
+                  Requires<[IsLP64]>;
+
+let Defs = [EAX, EFLAGS],
+    Uses = [ESP, EDI],
+    usesCustomInserter = 1 in
+def TLSCall_X32: I<0, Pseudo, (outs), (ins i32mem:$sym),
+                  "# TLSCall_64",
+                  [(X86TLSCall addr:$sym)]>,
+                  Requires<[IsX32]>;
 
 
 //===----------------------------------------------------------------------===//
@@ -1001,21 +1053,32 @@ def : Pat<(store (i64 (X86Wrapper tblockaddress:$src)), addr:$dst),
 // tls has some funny stuff here...
 // This corresponds to movabs $foo@tpoff, %rax
 def : Pat<(i64 (X86Wrapper tglobaltlsaddr :$dst)),
-          (MOV64ri tglobaltlsaddr :$dst)>;
+          (MOV64ri tglobaltlsaddr :$dst)>, Requires<[IsLP64]>;
+def : Pat<(i32 (X86Wrapper tglobaltlsaddr :$dst)),
+          (MOV32ri tglobaltlsaddr :$dst)>, Requires<[IsX32]>;
 // This corresponds to add $foo@tpoff, %rax
 def : Pat<(add GR64:$src1, (X86Wrapper tglobaltlsaddr :$dst)),
-          (ADD64ri32 GR64:$src1, tglobaltlsaddr :$dst)>;
+          (ADD64ri32 GR64:$src1, tglobaltlsaddr :$dst)>, Requires<[IsLP64]>;
+def : Pat<(add GR32:$src1, (X86Wrapper tglobaltlsaddr :$dst)),
+          (ADD32ri GR32:$src1, tglobaltlsaddr :$dst)>, Requires<[IsX32]>;
 // This corresponds to mov foo@tpoff(%rbx), %eax
 def : Pat<(load (i64 (X86Wrapper tglobaltlsaddr :$dst))),
-          (MOV64rm tglobaltlsaddr :$dst)>;
+          (MOV64rm tglobaltlsaddr :$dst)>, Requires<[IsLP64]>;
+def : Pat<(load (i32 (X86Wrapper tglobaltlsaddr :$dst))),
+          (MOV32rm tglobaltlsaddr :$dst)>, Requires<[IsX32]>;
 
 
 // Direct PC relative function call for small code model. 32-bit displacement
 // sign extended to 64-bit.
 def : Pat<(X86call (i64 tglobaladdr:$dst)),
-          (CALL64pcrel32 tglobaladdr:$dst)>;
+          (CALL64pcrel32 tglobaladdr:$dst)>, Requires<[IsLP64]>;
 def : Pat<(X86call (i64 texternalsym:$dst)),
-          (CALL64pcrel32 texternalsym:$dst)>;
+          (CALL64pcrel32 texternalsym:$dst)>, Requires<[IsLP64]>;
+
+def : Pat<(X86call (i32 tglobaladdr:$dst)),
+          (CALLX32pcrel32 tglobaladdr:$dst)>, Requires<[IsX32]>;
+def : Pat<(X86call (i32 texternalsym:$dst)),
+          (CALLX32pcrel32 texternalsym:$dst)>, Requires<[IsX32]>;
 
 // tailcall stuff
 def : Pat<(X86tcret ptr_rc_tailcall:$dst, imm:$off),
@@ -1039,25 +1102,37 @@ def : Pat<(X86tcret (i32 texternalsym:$dst), imm:$off),
 
 def : Pat<(X86tcret ptr_rc_tailcall:$dst, imm:$off),
           (TCRETURNri64 ptr_rc_tailcall:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+          Requires<[IsLP64]>;
 
 def : Pat<(X86tcret (load addr:$dst), imm:$off),
           (TCRETURNmi64 addr:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+          Requires<[IsLP64]>;
 
 def : Pat<(X86tcret (i64 tglobaladdr:$dst), imm:$off),
           (TCRETURNdi64 tglobaladdr:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+          Requires<[IsLP64]>;
 
 def : Pat<(X86tcret (i64 texternalsym:$dst), imm:$off),
           (TCRETURNdi64 texternalsym:$dst, imm:$off)>,
-          Requires<[In64BitMode]>;
+          Requires<[IsLP64]>;
+
+def : Pat<(X86tcret ptr_rc_tailcall:$dst, imm:$off),
+          (TCRETURNriX32 ptr_rc_tailcall:$dst, imm:$off)>,
+          Requires<[IsX32]>;
+
+def : Pat<(X86tcret (i32 tglobaladdr:$dst), imm:$off),
+          (TCRETURNdiX32 tglobaladdr:$dst, imm:$off)>,
+          Requires<[IsX32]>;
+
+def : Pat<(X86tcret (i32 texternalsym:$dst), imm:$off),
+          (TCRETURNdiX32 texternalsym:$dst, imm:$off)>,
+          Requires<[IsX32]>;
 
 // Normal calls, with various flavors of addresses.
 def : Pat<(X86call (i32 tglobaladdr:$dst)),
-          (CALLpcrel32 tglobaladdr:$dst)>;
+          (CALLpcrel32 tglobaladdr:$dst)>, Requires<[In32BitMode]>;
 def : Pat<(X86call (i32 texternalsym:$dst)),
-          (CALLpcrel32 texternalsym:$dst)>;
+          (CALLpcrel32 texternalsym:$dst)>, Requires<[In32BitMode]>;
 def : Pat<(X86call (i32 imm:$dst)),
           (CALLpcrel32 imm:$dst)>, Requires<[CallImmAddr]>;
 
diff --git a/lib/Target/X86/X86InstrControl.td b/lib/Target/X86/X86InstrControl.td
index 028f364..6c0350e 100644
--- a/lib/Target/X86/X86InstrControl.td
+++ b/lib/Target/X86/X86InstrControl.td
@@ -116,6 +116,10 @@ let isBranch = 1, isTerminator = 1, isBarrier = 1, isIndirectBranch = 1 in {
   def JMP64m     : I<0xFF, MRM4m, (outs), (ins i64mem:$dst), "jmp{q}\t{*}$dst",
                      [(brind (loadi64 addr:$dst))], IIC_JMP_MEM>, Requires<[In64BitMode]>;
 
+  let isCodeGenOnly = 1 in
+    def JMPX32r  : I<0xFF, MRM4r, (outs), (ins GR32:$dst), "jmp{q}\t{*}$dst",
+                     [(brind GR32:$dst)]>, Requires<[IsX32]>;
+
   def FARJMP16i  : Iseg16<0xEA, RawFrmImm16, (outs),
                           (ins i16imm:$off, i16imm:$seg),
                           "ljmp{w}\t{$seg, $off|$off, $seg}", [], IIC_JMP_FAR_PTR>, OpSize;
@@ -233,6 +237,27 @@ let isCall = 1, Uses = [RSP] in {
   def FARCALL64   : RI<0xFF, MRM3m, (outs), (ins opaque80mem:$dst),
                        "lcall{q}\t{*}$dst", [], IIC_CALL_FAR_MEM>;
 }
+let isCall = 1, isCodeGenOnly = 1, Uses = [RSP] in {
+  // All calls clobber the non-callee saved registers. RSP is marked as
+  // a use to prevent stack-pointer assignments that appear immediately
+  // before calls from potentially appearing dead. Uses for argument
+  // registers are added manually.
+  def CALLX32pcrel32: Ii32PCRel<0xE8, RawFrm,
+                        (outs), (ins i32imm_pcrel:$dst, variable_ops),
+                        "call{q}\t$dst", [], IIC_CALL_RI>,
+                      Requires<[IsX32]>;
+  def CALLX32r      : I<0xFF, MRM2r, (outs), (ins GR32:$dst, variable_ops),
+                        "call{q}\t{*}$dst", [(X86call GR32:$dst)],
+                        IIC_CALL_RI>,
+                      Requires<[IsX32]>;
+  let usesCustomInserter = 1 in
+    def CALLX32m    : PseudoI<(outs), (ins i32mem:$dst, variable_ops),
+                              [(X86call (loadi32 addr:$dst))]>,
+                      Requires<[IsX32]>;
+
+  def FARCALLX32    : RI<0xFF, MRM3m, (outs), (ins opaque80mem:$dst),
+                         "lcall{q}\t{*}$dst", [], IIC_CALL_FAR_MEM>;
+}
 
 let isCall = 1, isCodeGenOnly = 1 in
   // __chkstk(MSVC):     clobber R10, R11 and EFLAGS.
@@ -267,4 +292,16 @@ let isCall = 1, isTerminator = 1, isReturn = 1, isBarrier = 1,
   let mayLoad = 1 in
   def TAILJMPm64 : I<0xFF, MRM4m, (outs), (ins i64mem_TC:$dst, variable_ops),
                      "jmp{q}\t{*}$dst  # TAILCALL", [], IIC_JMP_MEM>;
+
+  def TCRETURNdiX32: PseudoI<(outs),
+                      (ins i32imm_pcrel:$dst, i32imm:$offset, variable_ops),
+                      []>;
+  def TCRETURNriX32: PseudoI<(outs),
+                      (ins ptr_rc_tailcall:$dst, i32imm:$offset, variable_ops), []>;
+
+  def TAILJMPdX32: Ii32PCRel<0xE9, RawFrm, (outs),
+                             (ins i32imm_pcrel:$dst, variable_ops),
+                             "jmp\t$dst  # TAILCALL", []>;
+  def TAILJMPrX32: I<0xFF, MRM4r, (outs), (ins ptr_rc_tailcall:$dst, variable_ops),
+                     "jmp{q}\t{*}$dst  # TAILCALL", []>;
 }
diff --git a/lib/Target/X86/X86InstrInfo.cpp b/lib/Target/X86/X86InstrInfo.cpp
index b8c29ac..bea519e 100644
--- a/lib/Target/X86/X86InstrInfo.cpp
+++ b/lib/Target/X86/X86InstrInfo.cpp
@@ -92,10 +92,14 @@ struct X86OpTblEntry {
 
 X86InstrInfo::X86InstrInfo(X86TargetMachine &tm)
   : X86GenInstrInfo((tm.getSubtarget<X86Subtarget>().is64Bit()
-                     ? X86::ADJCALLSTACKDOWN64
+                     ? tm.getSubtarget<X86Subtarget>().isX32()
+                       ? X86::ADJCALLSTACKDOWNX32
+                       : X86::ADJCALLSTACKDOWN64
                      : X86::ADJCALLSTACKDOWN32),
                     (tm.getSubtarget<X86Subtarget>().is64Bit()
-                     ? X86::ADJCALLSTACKUP64
+                     ? tm.getSubtarget<X86Subtarget>().isX32()
+                       ? X86::ADJCALLSTACKUPX32
+                       : X86::ADJCALLSTACKUP64
                      : X86::ADJCALLSTACKUP32)),
     TM(tm), RI(tm, *this) {
 
@@ -1537,6 +1541,7 @@ X86InstrInfo::isReallyTriviallyReMaterializable(const MachineInstr *MI,
       return false;
     }
 
+     case X86::LEAX32r:
      case X86::LEA32r:
      case X86::LEA64r: {
        if (MI->getOperand(2).isImm() &&
@@ -1699,6 +1704,10 @@ static bool hasLiveCondCodeDef(MachineInstr *MI) {
   return false;
 }
 
+static unsigned getLEA32Opcode(const X86Subtarget& STI) {
+  return STI.isLP64() ? X86::LEA64_32r : STI.isX32() ? X86::LEAX32r : X86::LEA32r;
+}
+
 /// convertToThreeAddressWithLEA - Helper for convertToThreeAddress when
 /// 16-bit LEA is disabled, use 32-bit LEA to form 3-address code by promoting
 /// to a 32-bit superregister and then truncating back down to a 16-bit
@@ -1714,8 +1723,7 @@ X86InstrInfo::convertToThreeAddressWithLEA(unsigned MIOpc,
   bool isDead = MI->getOperand(0).isDead();
   bool isKill = MI->getOperand(1).isKill();
 
-  unsigned Opc = TM.getSubtarget<X86Subtarget>().is64Bit()
-    ? X86::LEA64_32r : X86::LEA32r;
+  unsigned Opc = getLEA32Opcode(TM.getSubtarget<X86Subtarget>());
   MachineRegisterInfo &RegInfo = MFI->getParent()->getRegInfo();
   unsigned leaInReg = RegInfo.createVirtualRegister(&X86::GR32_NOSPRegClass);
   unsigned leaOutReg = RegInfo.createVirtualRegister(&X86::GR32RegClass);
@@ -1898,7 +1906,7 @@ X86InstrInfo::convertToThreeAddress(MachineFunction::iterator &MFI,
         !MF.getRegInfo().constrainRegClass(Src, &X86::GR32_NOSPRegClass))
       return 0;
 
-    unsigned Opc = is64Bit ? X86::LEA64_32r : X86::LEA32r;
+    unsigned Opc = getLEA32Opcode(TM.getSubtarget<X86Subtarget>());
     NewMI = BuildMI(MF, MI->getDebugLoc(), get(Opc))
       .addReg(Dest, RegState::Define | getDeadRegState(isDead))
       .addReg(0).addImm(1 << ShAmt)
@@ -1934,8 +1942,8 @@ X86InstrInfo::convertToThreeAddress(MachineFunction::iterator &MFI,
     case X86::INC32r:
     case X86::INC64_32r: {
       assert(MI->getNumOperands() >= 2 && "Unknown inc instruction!");
-      unsigned Opc = MIOpc == X86::INC64r ? X86::LEA64r
-        : (is64Bit ? X86::LEA64_32r : X86::LEA32r);
+      unsigned Opc = MIOpc == X86::INC64r ? unsigned(X86::LEA64r)
+        : getLEA32Opcode(TM.getSubtarget<X86Subtarget>());
       const TargetRegisterClass *RC = MIOpc == X86::INC64r ?
         (const TargetRegisterClass*)&X86::GR64_NOSPRegClass :
         (const TargetRegisterClass*)&X86::GR32_NOSPRegClass;
@@ -1965,8 +1973,8 @@ X86InstrInfo::convertToThreeAddress(MachineFunction::iterator &MFI,
     case X86::DEC32r:
     case X86::DEC64_32r: {
       assert(MI->getNumOperands() >= 2 && "Unknown dec instruction!");
-      unsigned Opc = MIOpc == X86::DEC64r ? X86::LEA64r
-        : (is64Bit ? X86::LEA64_32r : X86::LEA32r);
+      unsigned Opc = MIOpc == X86::DEC64r ? unsigned(X86::LEA64r)
+        : getLEA32Opcode(TM.getSubtarget<X86Subtarget>());
       const TargetRegisterClass *RC = MIOpc == X86::DEC64r ?
         (const TargetRegisterClass*)&X86::GR64_NOSPRegClass :
         (const TargetRegisterClass*)&X86::GR32_NOSPRegClass;
@@ -2002,7 +2010,7 @@ X86InstrInfo::convertToThreeAddress(MachineFunction::iterator &MFI,
         Opc = X86::LEA64r;
         RC = &X86::GR64_NOSPRegClass;
       } else {
-        Opc = is64Bit ? X86::LEA64_32r : X86::LEA32r;
+        Opc = getLEA32Opcode(TM.getSubtarget<X86Subtarget>());
         RC = &X86::GR32_NOSPRegClass;
       }
 
@@ -2053,7 +2061,7 @@ X86InstrInfo::convertToThreeAddress(MachineFunction::iterator &MFI,
     case X86::ADD32ri_DB:
     case X86::ADD32ri8_DB: {
       assert(MI->getNumOperands() >= 3 && "Unknown add instruction!");
-      unsigned Opc = is64Bit ? X86::LEA64_32r : X86::LEA32r;
+      unsigned Opc = getLEA32Opcode(TM.getSubtarget<X86Subtarget>());
       NewMI = addRegOffset(BuildMI(MF, MI->getDebugLoc(), get(Opc))
                               .addReg(Dest, RegState::Define |
                                       getDeadRegState(isDead)),
@@ -2617,8 +2625,13 @@ void X86InstrInfo::copyPhysReg(MachineBasicBlock &MBB,
       BuildMI(MBB, MI, DL, get(X86::POP64r), DestReg);
       return;
     } else if (X86::GR32RegClass.contains(DestReg)) {
-      BuildMI(MBB, MI, DL, get(X86::PUSHF32));
-      BuildMI(MBB, MI, DL, get(X86::POP32r), DestReg);
+      if (TM.getSubtarget<X86Subtarget>().isX32()) {
+        BuildMI(MBB, MI, DL, get(X86::PUSHF64));
+        BuildMI(MBB, MI, DL, get(X86::POPX32r), DestReg);
+      } else {
+        BuildMI(MBB, MI, DL, get(X86::PUSHF32));
+        BuildMI(MBB, MI, DL, get(X86::POP32r), DestReg);
+      }
       return;
     }
   }
@@ -2629,9 +2642,15 @@ void X86InstrInfo::copyPhysReg(MachineBasicBlock &MBB,
       BuildMI(MBB, MI, DL, get(X86::POPF64));
       return;
     } else if (X86::GR32RegClass.contains(SrcReg)) {
-      BuildMI(MBB, MI, DL, get(X86::PUSH32r))
-        .addReg(SrcReg, getKillRegState(KillSrc));
-      BuildMI(MBB, MI, DL, get(X86::POPF32));
+      if (TM.getSubtarget<X86Subtarget>().isX32()) {
+        BuildMI(MBB, MI, DL, get(X86::PUSHX32r))
+          .addReg(SrcReg, getKillRegState(KillSrc));
+        BuildMI(MBB, MI, DL, get(X86::POPF64));
+      } else {
+        BuildMI(MBB, MI, DL, get(X86::PUSH32r))
+          .addReg(SrcReg, getKillRegState(KillSrc));
+        BuildMI(MBB, MI, DL, get(X86::POPF32));
+      }
       return;
     }
   }
@@ -4098,6 +4117,7 @@ namespace {
         switch (I->getOpcode()) {
           case X86::TLS_base_addr32:
           case X86::TLS_base_addr64:
+          case X86::TLS_base_addrX32:
             if (TLSBaseAddrReg)
               I = ReplaceTLSBaseAddrCall(I, TLSBaseAddrReg);
             else
@@ -4125,13 +4145,13 @@ namespace {
       MachineFunction *MF = I->getParent()->getParent();
       const X86TargetMachine *TM =
           static_cast<const X86TargetMachine *>(&MF->getTarget());
-      const bool is64Bit = TM->getSubtarget<X86Subtarget>().is64Bit();
+      const bool IsLP64 = TM->getSubtarget<X86Subtarget>().isLP64();
       const X86InstrInfo *TII = TM->getInstrInfo();
 
       // Insert a Copy from TLSBaseAddrReg to RAX/EAX.
       MachineInstr *Copy = BuildMI(*I->getParent(), I, I->getDebugLoc(),
                                    TII->get(TargetOpcode::COPY),
-                                   is64Bit ? X86::RAX : X86::EAX)
+                                   IsLP64 ? X86::RAX : X86::EAX)
                                    .addReg(TLSBaseAddrReg);
 
       // Erase the TLS_base_addr instruction.
@@ -4146,12 +4166,12 @@ namespace {
       MachineFunction *MF = I->getParent()->getParent();
       const X86TargetMachine *TM =
           static_cast<const X86TargetMachine *>(&MF->getTarget());
-      const bool is64Bit = TM->getSubtarget<X86Subtarget>().is64Bit();
+      const bool IsLP64 = TM->getSubtarget<X86Subtarget>().isLP64();
       const X86InstrInfo *TII = TM->getInstrInfo();
 
       // Create a virtual register for the TLS base address.
       MachineRegisterInfo &RegInfo = MF->getRegInfo();
-      *TLSBaseAddrReg = RegInfo.createVirtualRegister(is64Bit
+      *TLSBaseAddrReg = RegInfo.createVirtualRegister(IsLP64
                                                       ? &X86::GR64RegClass
                                                       : &X86::GR32RegClass);
 
@@ -4160,7 +4180,7 @@ namespace {
       MachineInstr *Copy = BuildMI(*I->getParent(), Next, I->getDebugLoc(),
                                    TII->get(TargetOpcode::COPY),
                                    *TLSBaseAddrReg)
-                                   .addReg(is64Bit ? X86::RAX : X86::EAX);
+                                   .addReg(IsLP64 ? X86::RAX : X86::EAX);
 
       return Copy;
     }
diff --git a/lib/Target/X86/X86InstrInfo.td b/lib/Target/X86/X86InstrInfo.td
index 9ce6140..ca5aef0 100644
--- a/lib/Target/X86/X86InstrInfo.td
+++ b/lib/Target/X86/X86InstrInfo.td
@@ -327,11 +327,15 @@ def f256mem : X86MemOperand<"printf256mem">{
   let ParserMatchClass = X86Mem256AsmOperand; }
 }
 
+// GPRs available for i8mem doesn't requires a REX prefix.
+def ptr_rc_norex : PointerLikeRegClass<3>;
+def ptr_rc_norex_nosp : PointerLikeRegClass<4>;
+
 // A version of i8mem for use on x86-64 that uses GR64_NOREX instead of
 // plain GR64, so that it doesn't potentially require a REX prefix.
-def i8mem_NOREX : Operand<i64> {
+def i8mem_NOREX : Operand<iPTR> {
   let PrintMethod = "printi8mem";
-  let MIOperandInfo = (ops GR64_NOREX, i8imm, GR64_NOREX_NOSP, i32imm, i8imm);
+  let MIOperandInfo = (ops ptr_rc_norex, i8imm, ptr_rc_norex_nosp, i32imm, i8imm);
   let ParserMatchClass = X86Mem8AsmOperand;
   let OperandType = "OPERAND_MEMORY";
 }
@@ -484,6 +488,12 @@ def lea64_32mem : Operand<i32> {
   let ParserMatchClass = X86MemAsmOperand;
 }
 
+def leax32_64mem : Operand<i64> {
+  let PrintMethod = "printi64mem";
+  let MIOperandInfo = (ops GR64, i8imm, GR64_NOSP, i32imm, i8imm);
+  let ParserMatchClass = X86MemAsmOperand;
+}
+
 
 //===----------------------------------------------------------------------===//
 // X86 Complex Pattern Definitions.
@@ -510,6 +520,10 @@ def tls64addr : ComplexPattern<i64, 5, "SelectTLSADDRAddr",
 def tls64baseaddr : ComplexPattern<i64, 5, "SelectTLSADDRAddr",
                                [tglobaltlsaddr], []>;
 
+def leax32addr: ComplexPattern<i32, 5, "SelectLEAAddr",
+                        [add, sub, mul, X86mul_imm, shl, or, frameindex,
+                         X86WrapperRIP], []>;
+
 //===----------------------------------------------------------------------===//
 // X86 Instruction Predicate Definitions.
 def HasCMov      : Predicate<"Subtarget->hasCMov()">;
@@ -548,6 +562,8 @@ def In32BitMode  : Predicate<"!Subtarget->is64Bit()">,
                              AssemblerPredicate<"!Mode64Bit">;
 def In64BitMode  : Predicate<"Subtarget->is64Bit()">,
                              AssemblerPredicate<"Mode64Bit">;
+def IsX32        : Predicate<"Subtarget->isX32()">;
+def IsLP64       : Predicate<"Subtarget->isLP64()">;
 def IsWin64      : Predicate<"Subtarget->isTargetWin64()">;
 def IsNaCl       : Predicate<"Subtarget->isTargetNaCl()">;
 def NotNaCl      : Predicate<"!Subtarget->isTargetNaCl()">;
@@ -783,7 +799,7 @@ def POP64rmm: I<0x8F, MRM0m, (outs i64mem:$dst), (ins), "pop{q}\t$dst", [],
 }
 let mayStore = 1 in {
 def PUSH64r  : I<0x50, AddRegFrm,
-                 (outs), (ins GR64:$reg), "push{q}\t$reg", [], IIC_PUSH_REG>;
+                 (outs), (ins GR64:$reg), "push{q}\t$reg", [],IIC_PUSH_REG>;
 def PUSH64rmr: I<0xFF, MRM6r, (outs), (ins GR64:$reg), "push{q}\t$reg", [],
                  IIC_PUSH_REG>;
 def PUSH64rmm: I<0xFF, MRM6m, (outs), (ins i64mem:$src), "push{q}\t$src", [],
@@ -807,6 +823,15 @@ let Defs = [RSP], Uses = [RSP, EFLAGS], mayStore = 1, neverHasSideEffects=1 in
 def PUSHF64    : I<0x9C, RawFrm, (outs), (ins), "pushfq", [], IIC_PUSH_F>,
                  Requires<[In64BitMode]>;
 
+let Defs = [ESP], Uses = [ESP], neverHasSideEffects=1, isCodeGenOnly=1 in {
+let mayLoad = 1 in
+  def POPX32r: I<0x58, AddRegFrm,
+                 (outs GR32:$reg), (ins), "pop{q}\t$reg", [], IIC_POP_REG>;
+let mayStore = 1 in
+  def PUSHX32r: I<0x50, AddRegFrm,
+                  (outs), (ins GR32:$reg), "push{q}\t$reg", [], IIC_PUSH_REG>;
+}
+
 
 
 let Defs = [EDI, ESI, EBP, EBX, EDX, ECX, EAX, ESP], Uses = [ESP],
diff --git a/lib/Target/X86/X86JITInfo.cpp b/lib/Target/X86/X86JITInfo.cpp
index 0168d12..4929ffc 100644
--- a/lib/Target/X86/X86JITInfo.cpp
+++ b/lib/Target/X86/X86JITInfo.cpp
@@ -26,7 +26,11 @@ using namespace llvm;
 
 // Determine the platform we're running on
 #if defined (__x86_64__) || defined (_M_AMD64) || defined (_M_X64)
-# define X86_64_JIT
+#  if defined(__x86_64__) && defined(__ILP32__)
+#   define X86_X32_JIT
+# else
+#   define X86_64_JIT
+# endif
 #elif defined(__i386__) || defined(i386) || defined(_M_IX86)
 # define X86_32_JIT
 #endif
@@ -185,6 +189,94 @@ extern "C" {
   void X86CompilationCallback();
 
 # endif
+#elif defined (X86_X32_JIT)
+  // No need to save EAX/EDX for X86-64.
+  void X86CompilationCallback(void);
+  asm(
+    ".text\n"
+    ".align 8\n"
+    ".globl " ASMPREFIX "X86CompilationCallback\n"
+    TYPE_FUNCTION(X86CompilationCallback)
+  ASMPREFIX "X86CompilationCallback:\n"
+    CFI(".cfi_startproc\n")
+    // Save EBP
+    "pushq   %rbp\n"
+    CFI(".cfi_def_cfa_offset 16\n")
+    CFI(".cfi_offset %rbp, -16\n")
+    // Save ESP
+    "mov     %esp, %ebp\n"
+    CFI(".cfi_def_cfa_register %rbp\n")
+    // Save all int arg registers
+    "pushq   %rdi\n"
+    CFI(".cfi_rel_offset %rdi, 0\n")
+    "pushq   %rsi\n"
+    CFI(".cfi_rel_offset %rsi, 8\n")
+    "pushq   %rdx\n"
+    CFI(".cfi_rel_offset %rdx, 16\n")
+    "pushq   %rcx\n"
+    CFI(".cfi_rel_offset %rcx, 24\n")
+    "pushq   %r8\n"
+    CFI(".cfi_rel_offset %r8, 32\n")
+    "pushq   %r9\n"
+    CFI(".cfi_rel_offset %r9, 40\n")
+    // Align stack on 16-byte boundary. ESP might not be properly aligned
+    // (8 byte) if this is called from an indirect stub.
+    "and     $-16, %esp\n"
+    // Save all XMM arg registers
+    "sub     $128, %esp\n"
+    "movaps  %xmm0, (%esp)\n"
+    "movaps  %xmm1, 16(%esp)\n"
+    "movaps  %xmm2, 32(%esp)\n"
+    "movaps  %xmm3, 48(%esp)\n"
+    "movaps  %xmm4, 64(%esp)\n"
+    "movaps  %xmm5, 80(%esp)\n"
+    "movaps  %xmm6, 96(%esp)\n"
+    "movaps  %xmm7, 112(%esp)\n"
+    // JIT callee
+    "mov     %ebp, %edi\n"    // Pass prev frame and return address
+    "mov     8(%ebp), %esi\n"
+    "call    " ASMPREFIX "X86CompilationCallback2\n"
+    // Restore all XMM arg registers
+    "movaps  112(%esp), %xmm7\n"
+    "movaps  96(%esp), %xmm6\n"
+    "movaps  80(%esp), %xmm5\n"
+    "movaps  64(%esp), %xmm4\n"
+    "movaps  48(%esp), %xmm3\n"
+    "movaps  32(%esp), %xmm2\n"
+    "movaps  16(%esp), %xmm1\n"
+    "movaps  (%esp), %xmm0\n"
+    // Restore ESP
+    "mov     %ebp, %esp\n"
+    CFI(".cfi_def_cfa_register %rsp\n")
+    // Restore all int arg registers
+    "sub     $48, %esp\n"
+    CFI(".cfi_adjust_cfa_offset 48\n")
+    "popq    %r9\n"
+    CFI(".cfi_adjust_cfa_offset -8\n")
+    CFI(".cfi_restore %r9\n")
+    "popq    %r8\n"
+    CFI(".cfi_adjust_cfa_offset -8\n")
+    CFI(".cfi_restore %r8\n")
+    "popq    %rcx\n"
+    CFI(".cfi_adjust_cfa_offset -8\n")
+    CFI(".cfi_restore %rcx\n")
+    "popq    %rdx\n"
+    CFI(".cfi_adjust_cfa_offset -8\n")
+    CFI(".cfi_restore %rdx\n")
+    "popq    %rsi\n"
+    CFI(".cfi_adjust_cfa_offset -8\n")
+    CFI(".cfi_restore %rsi\n")
+    "popq    %rdi\n"
+    CFI(".cfi_adjust_cfa_offset -8\n")
+    CFI(".cfi_restore %rdi\n")
+    // Restore EBP
+    "popq    %rbp\n"
+    CFI(".cfi_adjust_cfa_offset -8\n")
+    CFI(".cfi_restore %rbp\n")
+    "ret\n"
+    CFI(".cfi_endproc\n")
+    SIZE(X86CompilationCallback)
+  );
 #elif defined (X86_32_JIT)
 # ifndef _MSC_VER
   void X86CompilationCallback(void);
@@ -350,7 +442,11 @@ static
 #endif
 void LLVM_ATTRIBUTE_USED
 X86CompilationCallback2(intptr_t *StackPtr, intptr_t RetAddr) {
+#if defined(X86_X32_JIT)
+  intptr_t *RetAddrLoc = &StackPtr[2];
+#else
   intptr_t *RetAddrLoc = &StackPtr[1];
+#endif
   assert(*RetAddrLoc == RetAddr &&
          "Could not find return address on the stack!");
 
@@ -358,7 +454,7 @@ X86CompilationCallback2(intptr_t *StackPtr, intptr_t RetAddr) {
   bool isStub = ((unsigned char*)RetAddr)[0] == 0xCE;
 
   // The call instruction should have pushed the return value onto the stack...
-#if defined (X86_64_JIT)
+#if defined (X86_64_JIT) || defined(X86_X32_JIT)
   RetAddr--;     // Backtrack to the reference itself...
 #else
   RetAddr -= 4;  // Backtrack to the reference itself...
@@ -372,7 +468,7 @@ X86CompilationCallback2(intptr_t *StackPtr, intptr_t RetAddr) {
 #endif
 
   // Sanity check to make sure this really is a call instruction.
-#if defined (X86_64_JIT)
+#if defined (X86_64_JIT) || defined(X86_X32_JIT)
   assert(((unsigned char*)RetAddr)[-2] == 0x41 &&"Not a call instr!");
   assert(((unsigned char*)RetAddr)[-1] == 0xFF &&"Not a call instr!");
 #else
@@ -383,7 +479,7 @@ X86CompilationCallback2(intptr_t *StackPtr, intptr_t RetAddr) {
 
   // Rewrite the call target... so that we don't end up here every time we
   // execute the call.
-#if defined (X86_64_JIT)
+#if defined (X86_64_JIT) || defined (X86_X32_JIT)
   assert(isStub &&
          "X86-64 doesn't support rewriting non-stub lazy compilation calls:"
          " the call instruction varies too much.");
@@ -410,6 +506,10 @@ X86CompilationCallback2(intptr_t *StackPtr, intptr_t RetAddr) {
       ((unsigned char*)RetAddr)[0] = (2 | (4 << 3) | (3 << 6));
     }
     sys::ValgrindDiscardTranslations((void*)(RetAddr-0xc), 0xd);
+#elif defined (X86_X32_JIT)
+    *(intptr_t *)(RetAddr - 6) = NewVal;
+    ((unsigned char*)RetAddr)[0] = (2 | (4 << 3) | (3 << 6));
+    sys::ValgrindDiscardTranslations((void*)(RetAddr-8), 0xd);
 #else
     ((unsigned char*)RetAddr)[-1] = 0xE9;
     sys::ValgrindDiscardTranslations((void*)(RetAddr-1), 5);
@@ -419,6 +519,8 @@ X86CompilationCallback2(intptr_t *StackPtr, intptr_t RetAddr) {
   // Change the return address to reexecute the call instruction...
 #if defined (X86_64_JIT)
   *RetAddrLoc -= 0xd;
+#elif defined (X86_X32_JIT)
+  *RetAddrLoc -= 9;
 #else
   *RetAddrLoc -= 5;
 #endif
@@ -494,6 +596,13 @@ void *X86JITInfo::emitFunctionStub(const Function* F, void *Target,
     JCE.emitByte(0x41);          // REX prefix
     JCE.emitByte(0xFF);          // jmpq *r10
     JCE.emitByte(2 | (4 << 3) | (3 << 6));
+#elif defined (X86_X32_JIT)
+    JCE.emitByte(0x41);          // REX prefix
+    JCE.emitByte(0xB8+2);        // movabsl r10
+    JCE.emitWordLE((unsigned)(intptr_t)Target);
+    JCE.emitByte(0x41);          // REX prefix
+    JCE.emitByte(0xFF);          // jmpq *r10
+    JCE.emitByte(2 | (4 << 3) | (3 << 6));
 #else
     JCE.emitByte(0xE9);
     JCE.emitWordLE((intptr_t)Target-JCE.getCurrentPCValue()-4);
@@ -509,6 +618,13 @@ void *X86JITInfo::emitFunctionStub(const Function* F, void *Target,
   JCE.emitByte(0x41);          // REX prefix
   JCE.emitByte(0xFF);          // callq *r10
   JCE.emitByte(2 | (2 << 3) | (3 << 6));
+#elif defined (X86_X32_JIT)
+  JCE.emitByte(0x41);          // REX prefix
+  JCE.emitByte(0xB8+2);        // movabsl r10
+  JCE.emitWordLE((unsigned)(intptr_t)Target);
+  JCE.emitByte(0x41);          // REX prefix
+  JCE.emitByte(0xFF);          // callq *r10
+  JCE.emitByte(2 | (2 << 3) | (3 << 6));
 #else
   JCE.emitByte(0xE8);   // Call with 32 bit pc-rel destination...
 
diff --git a/lib/Target/X86/X86MCInstLower.cpp b/lib/Target/X86/X86MCInstLower.cpp
index 9dc5c70..caa2ed5 100644
--- a/lib/Target/X86/X86MCInstLower.cpp
+++ b/lib/Target/X86/X86MCInstLower.cpp
@@ -219,6 +219,13 @@ static void lower_lea64_32mem(MCInst *MI, unsigned OpNo) {
   }
 }
 
+static void lower_x32reg(MCInst *MI, unsigned OpNo) {
+  // convert registers to reg64
+  unsigned Reg = MI->getOperand(OpNo).getReg();
+  if (Reg != 0)
+    MI->getOperand(OpNo).setReg(getX86SubSuperRegister(Reg, MVT::i64));
+}
+
 /// LowerSubReg32_Op0 - Things like MOVZX16rr8 -> MOVZX32rr8.
 static void LowerSubReg32_Op0(MCInst &OutMI, unsigned NewOpc) {
   OutMI.setOpcode(NewOpc);
@@ -355,6 +362,8 @@ ReSimplify:
   case X86::LEA64_32r: // Handle 'subreg rewriting' for the lea64_32mem operand.
     lower_lea64_32mem(&OutMI, 1);
     // FALL THROUGH.
+  case X86::LEAX32_64r:
+  case X86::LEAX32r:
   case X86::LEA64r:
   case X86::LEA16r:
   case X86::LEA32r:
@@ -396,7 +405,11 @@ ReSimplify:
   // TAILJMPr64, CALL64r, CALL64pcrel32 - These instructions have register
   // inputs modeled as normal uses instead of implicit uses.  As such, truncate
   // off all but the first operand (the callee).  FIXME: Change isel.
+  case X86::TAILJMPrX32:
+  case X86::CALLX32r:
+      lower_x32reg(&OutMI, 0);
   case X86::TAILJMPr64:
+  case X86::CALLX32pcrel32:
   case X86::CALL64r:
   case X86::CALL64pcrel32: {
     unsigned Opcode = OutMI.getOpcode();
@@ -408,6 +421,7 @@ ReSimplify:
   }
 
   case X86::EH_RETURN:
+  case X86::EH_RETURNX32:
   case X86::EH_RETURN64: {
     OutMI = MCInst();
     OutMI.setOpcode(X86::RET);
@@ -417,12 +431,14 @@ ReSimplify:
   // TAILJMPd, TAILJMPd64 - Lower to the correct jump instructions.
   case X86::TAILJMPr:
   case X86::TAILJMPd:
+  case X86::TAILJMPdX32:
   case X86::TAILJMPd64: {
     unsigned Opcode;
     switch (OutMI.getOpcode()) {
     default: llvm_unreachable("Invalid opcode");
     case X86::TAILJMPr: Opcode = X86::JMP32r; break;
     case X86::TAILJMPd:
+    case X86::TAILJMPdX32:
     case X86::TAILJMPd64: Opcode = X86::JMP_1; break;
     }
     
@@ -433,6 +449,17 @@ ReSimplify:
     break;
   }
 
+  // PUSHX32r, POPX32r - Fix to 64-bit register in X32 mode.
+  case X86::PUSHX32r:
+  case X86::POPX32r: {
+    lower_x32reg(&OutMI, 0);
+    break;
+  }
+
+  case X86::JMPX32r:
+    lower_x32reg(&OutMI, 0);
+    break;
+
   // These are pseudo-ops for OR to help with the OR->ADD transformation.  We do
   // this with an ugly goto in case the resultant OR uses EAX and needs the
   // short form.
@@ -554,15 +581,23 @@ ReSimplify:
 static void LowerTlsAddr(MCStreamer &OutStreamer,
                          X86MCInstLower &MCInstLowering,
                          const MachineInstr &MI) {
+  MCContext &context = OutStreamer.getContext();
 
-  bool is64Bits = MI.getOpcode() == X86::TLS_addr64 ||
-                  MI.getOpcode() == X86::TLS_base_addr64;
+  bool is32Bits = MI.getOpcode() == X86::TLS_addr32 ||
+                  MI.getOpcode() == X86::TLS_base_addr32;
 
-  bool needsPadding = MI.getOpcode() == X86::TLS_addr64;
+  bool isLP64 = MI.getOpcode() == X86::TLS_addr64 ||
+                MI.getOpcode() == X86::TLS_base_addr64;
 
-  MCContext &context = OutStreamer.getContext();
+  bool isX32 = MI.getOpcode() == X86::TLS_addrX32 ||
+               MI.getOpcode() == X86::TLS_base_addrX32;
+
+  bool needsPaddingLEA = MI.getOpcode() == X86::TLS_addr64;
 
-  if (needsPadding) {
+  bool needsPaddingCALL = MI.getOpcode() == X86::TLS_addr64 ||
+                          MI.getOpcode() == X86::TLS_addrX32;
+
+  if (needsPaddingLEA) {
     MCInst prefix;
     prefix.setOpcode(X86::DATA16_PREFIX);
     OutStreamer.EmitInstruction(prefix);
@@ -572,12 +607,14 @@ static void LowerTlsAddr(MCStreamer &OutStreamer,
   switch (MI.getOpcode()) {
     case X86::TLS_addr32:
     case X86::TLS_addr64:
+    case X86::TLS_addrX32:
       SRVK = MCSymbolRefExpr::VK_TLSGD;
       break;
     case X86::TLS_base_addr32:
       SRVK = MCSymbolRefExpr::VK_TLSLDM;
       break;
     case X86::TLS_base_addr64:
+    case X86::TLS_base_addrX32:
       SRVK = MCSymbolRefExpr::VK_TLSLD;
       break;
     default:
@@ -588,7 +625,7 @@ static void LowerTlsAddr(MCStreamer &OutStreamer,
   const MCSymbolRefExpr *symRef = MCSymbolRefExpr::Create(sym, SRVK, context);
 
   MCInst LEA;
-  if (is64Bits) {
+  if (isLP64 || isX32) {
     LEA.setOpcode(X86::LEA64r);
     LEA.addOperand(MCOperand::CreateReg(X86::RDI)); // dest
     LEA.addOperand(MCOperand::CreateReg(X86::RIP)); // base
@@ -596,7 +633,7 @@ static void LowerTlsAddr(MCStreamer &OutStreamer,
     LEA.addOperand(MCOperand::CreateReg(0));        // index
     LEA.addOperand(MCOperand::CreateExpr(symRef));  // disp
     LEA.addOperand(MCOperand::CreateReg(0));        // seg
-  } else {
+  } else if (is32Bits) {
     LEA.setOpcode(X86::LEA32r);
     LEA.addOperand(MCOperand::CreateReg(X86::EAX)); // dest
     LEA.addOperand(MCOperand::CreateReg(0));        // base
@@ -604,10 +641,11 @@ static void LowerTlsAddr(MCStreamer &OutStreamer,
     LEA.addOperand(MCOperand::CreateReg(X86::EBX)); // index
     LEA.addOperand(MCOperand::CreateExpr(symRef));  // disp
     LEA.addOperand(MCOperand::CreateReg(0));        // seg
-  }
+  } else
+    llvm_unreachable("unexpected opcode");
   OutStreamer.EmitInstruction(LEA);
 
-  if (needsPadding) {
+  if (needsPaddingCALL) {
     MCInst prefix;
     prefix.setOpcode(X86::DATA16_PREFIX);
     OutStreamer.EmitInstruction(prefix);
@@ -618,11 +656,15 @@ static void LowerTlsAddr(MCStreamer &OutStreamer,
   }
 
   MCInst call;
-  if (is64Bits)
+  if (isLP64)
     call.setOpcode(X86::CALL64pcrel32);
-  else
+  else if (isX32)
+    call.setOpcode(X86::CALLX32pcrel32);
+  else if (is32Bits)
     call.setOpcode(X86::CALLpcrel32);
-  StringRef name = is64Bits ? "__tls_get_addr" : "___tls_get_addr";
+  else
+    llvm_unreachable("unexpected opcode");
+  StringRef name = (isLP64 || isX32) ? "__tls_get_addr" : "___tls_get_addr";
   MCSymbol *tlsGetAddr = context.GetOrCreateSymbol(name);
   const MCSymbolRefExpr *tlsRef =
     MCSymbolRefExpr::Create(tlsGetAddr,
@@ -653,6 +695,7 @@ void X86AsmPrinter::EmitInstruction(const MachineInstr *MI) {
 
 
   case X86::EH_RETURN:
+  case X86::EH_RETURNX32:
   case X86::EH_RETURN64: {
     // Lower these as normal, but add some comments.
     unsigned Reg = MI->getOperand(0).getReg();
@@ -662,6 +705,7 @@ void X86AsmPrinter::EmitInstruction(const MachineInstr *MI) {
   }
   case X86::TAILJMPr:
   case X86::TAILJMPd:
+  case X86::TAILJMPdX32:
   case X86::TAILJMPd64:
     // Lower these as normal, but add some comments.
     OutStreamer.AddComment("TAILCALL");
@@ -669,8 +713,10 @@ void X86AsmPrinter::EmitInstruction(const MachineInstr *MI) {
 
   case X86::TLS_addr32:
   case X86::TLS_addr64:
+  case X86::TLS_addrX32:
   case X86::TLS_base_addr32:
   case X86::TLS_base_addr64:
+  case X86::TLS_base_addrX32:
     return LowerTlsAddr(OutStreamer, MCInstLowering, *MI);
 
   case X86::MOVPC32r: {
diff --git a/lib/Target/X86/X86RegisterInfo.cpp b/lib/Target/X86/X86RegisterInfo.cpp
index b22a086..a733de2 100644
--- a/lib/Target/X86/X86RegisterInfo.cpp
+++ b/lib/Target/X86/X86RegisterInfo.cpp
@@ -66,8 +66,8 @@ X86RegisterInfo::X86RegisterInfo(X86TargetMachine &tm,
 
   if (Is64Bit) {
     SlotSize = 8;
-    StackPtr = X86::RSP;
-    FramePtr = X86::RBP;
+    StackPtr = Subtarget->isX32() ? X86::ESP : X86::RSP;
+    FramePtr = Subtarget->isX32() ? X86::EBP : X86::RBP;
   } else {
     SlotSize = 4;
     StackPtr = X86::ESP;
@@ -186,26 +186,38 @@ X86RegisterInfo::getPointerRegClass(const MachineFunction &MF, unsigned Kind)
   switch (Kind) {
   default: llvm_unreachable("Unexpected Kind in getPointerRegClass!");
   case 0: // Normal GPRs.
-    if (TM.getSubtarget<X86Subtarget>().is64Bit())
+    if (TM.getSubtarget<X86Subtarget>().isLP64())
       return &X86::GR64RegClass;
     return &X86::GR32RegClass;
   case 1: // Normal GPRs except the stack pointer (for encoding reasons).
-    if (TM.getSubtarget<X86Subtarget>().is64Bit())
+    if (TM.getSubtarget<X86Subtarget>().isLP64())
       return &X86::GR64_NOSPRegClass;
     return &X86::GR32_NOSPRegClass;
   case 2: // Available for tailcall (not callee-saved GPRs).
     if (TM.getSubtarget<X86Subtarget>().isTargetWin64())
       return &X86::GR64_TCW64RegClass;
-    if (TM.getSubtarget<X86Subtarget>().is64Bit())
+    if (TM.getSubtarget<X86Subtarget>().isLP64())
       return &X86::GR64_TCRegClass;
+    if (TM.getSubtarget<X86Subtarget>().isX32())
+      return &X86::GRX32_TCRegClass;
     return &X86::GR32_TCRegClass;
+  case 3: // Normal GPRs doesn't require a REX prefix
+    assert(TM.getSubtarget<X86Subtarget>().is64Bit() && "Only valid for x86-64!");
+    if (TM.getSubtarget<X86Subtarget>().isX32())
+      return &X86::GR32_NOREXRegClass;
+    return &X86::GR64_NOREXRegClass;
+  case 4: // Normal GPRs doesn't require a REX prefix except the stack pointer.
+    assert(TM.getSubtarget<X86Subtarget>().is64Bit() && "Only valid for x86-64!");
+    if (TM.getSubtarget<X86Subtarget>().isX32())
+      return &X86::GR32_NOREX_NOSPRegClass;
+    return &X86::GR64_NOREX_NOSPRegClass;
   }
 }
 
 const TargetRegisterClass *
 X86RegisterInfo::getCrossCopyRegClass(const TargetRegisterClass *RC) const {
   if (RC == &X86::CCRRegClass) {
-    if (Is64Bit)
+    if (TM.getSubtarget<X86Subtarget>().isLP64())
       return &X86::GR64RegClass;
     else
       return &X86::GR32RegClass;
@@ -371,15 +383,15 @@ bool X86RegisterInfo::hasReservedSpillSlot(const MachineFunction &MF,
                                            unsigned Reg, int &FrameIdx) const {
   const TargetFrameLowering *TFI = MF.getTarget().getFrameLowering();
 
-  if (Reg == FramePtr && TFI->hasFP(MF)) {
+  if (regsOverlap(Reg, FramePtr) && TFI->hasFP(MF)) {
     FrameIdx = MF.getFrameInfo()->getObjectIndexBegin();
     return true;
   }
   return false;
 }
 
-static unsigned getSUBriOpcode(unsigned is64Bit, int64_t Imm) {
-  if (is64Bit) {
+static unsigned getSUBriOpcode(unsigned IsLP64, int64_t Imm) {
+  if (IsLP64) {
     if (isInt<8>(Imm))
       return X86::SUB64ri8;
     return X86::SUB64ri32;
@@ -390,8 +402,8 @@ static unsigned getSUBriOpcode(unsigned is64Bit, int64_t Imm) {
   }
 }
 
-static unsigned getADDriOpcode(unsigned is64Bit, int64_t Imm) {
-  if (is64Bit) {
+static unsigned getADDriOpcode(unsigned IsLP64, int64_t Imm) {
+  if (IsLP64) {
     if (isInt<8>(Imm))
       return X86::ADD64ri8;
     return X86::ADD64ri32;
@@ -405,6 +417,7 @@ static unsigned getADDriOpcode(unsigned is64Bit, int64_t Imm) {
 void X86RegisterInfo::
 eliminateCallFramePseudoInstr(MachineFunction &MF, MachineBasicBlock &MBB,
                               MachineBasicBlock::iterator I) const {
+  const X86Subtarget *Subtarget = &TM.getSubtarget<X86Subtarget>();
   const TargetFrameLowering *TFI = MF.getTarget().getFrameLowering();
   bool reseveCallFrame = TFI->hasReservedCallFrame(MF);
   int Opcode = I->getOpcode();
@@ -430,7 +443,8 @@ eliminateCallFramePseudoInstr(MachineFunction &MF, MachineBasicBlock &MBB,
 
     MachineInstr *New = 0;
     if (Opcode == TII.getCallFrameSetupOpcode()) {
-      New = BuildMI(MF, DL, TII.get(getSUBriOpcode(Is64Bit, Amount)),
+      New = BuildMI(MF, DL, TII.get(getSUBriOpcode(Subtarget->isLP64(),
+                                                   Amount)),
                     StackPtr)
         .addReg(StackPtr)
         .addImm(Amount);
@@ -441,7 +455,7 @@ eliminateCallFramePseudoInstr(MachineFunction &MF, MachineBasicBlock &MBB,
       Amount -= CalleeAmt;
 
       if (Amount) {
-        unsigned Opc = getADDriOpcode(Is64Bit, Amount);
+        unsigned Opc = getADDriOpcode(Subtarget->isLP64(), Amount);
         New = BuildMI(MF, DL, TII.get(Opc), StackPtr)
           .addReg(StackPtr).addImm(Amount);
       }
@@ -462,7 +476,7 @@ eliminateCallFramePseudoInstr(MachineFunction &MF, MachineBasicBlock &MBB,
     // If we are performing frame pointer elimination and if the callee pops
     // something off the stack pointer, add it back.  We do this until we have
     // more advanced stack pointer tracking ability.
-    unsigned Opc = getSUBriOpcode(Is64Bit, CalleeAmt);
+    unsigned Opc = getSUBriOpcode(Subtarget->isLP64(), CalleeAmt);
     MachineInstr *New = BuildMI(MF, DL, TII.get(Opc), StackPtr)
       .addReg(StackPtr).addImm(CalleeAmt);
 
diff --git a/lib/Target/X86/X86RegisterInfo.td b/lib/Target/X86/X86RegisterInfo.td
index ae2d4d0..9a35ac3 100644
--- a/lib/Target/X86/X86RegisterInfo.td
+++ b/lib/Target/X86/X86RegisterInfo.td
@@ -337,6 +337,8 @@ def GR64_ABCD : RegisterClass<"X86", [i64], 64, (add RAX, RCX, RDX, RBX)>;
 def GR32_TC   : RegisterClass<"X86", [i32], 32, (add EAX, ECX, EDX)>;
 def GR64_TC   : RegisterClass<"X86", [i64], 64, (add RAX, RCX, RDX, RSI, RDI,
                                                      R8, R9, R11, RIP)>;
+def GRX32_TC  : RegisterClass<"X86", [i32], 32, (add EAX, ECX, EDX, ESI, EDI,
+                                                     R8D, R9D, R11D)>;
 def GR64_TCW64 : RegisterClass<"X86", [i64], 64, (add RAX, RCX, RDX,
                                                       R8, R9, R11)>;
 
diff --git a/lib/Target/X86/X86SelectionDAGInfo.cpp b/lib/Target/X86/X86SelectionDAGInfo.cpp
index 7c6788f..ff8d874 100644
--- a/lib/Target/X86/X86SelectionDAGInfo.cpp
+++ b/lib/Target/X86/X86SelectionDAGInfo.cpp
@@ -101,7 +101,7 @@ X86SelectionDAGInfo::EmitTargetCodeForMemset(SelectionDAG &DAG, DebugLoc dl,
       ValReg = X86::EAX;
       Val = (Val << 8)  | Val;
       Val = (Val << 16) | Val;
-      if (Subtarget->is64Bit() && ((Align & 0x7) == 0)) {  // QWORD aligned
+      if (Subtarget->isLP64() && ((Align & 0x7) == 0)) {  // QWORD aligned
         AVT = MVT::i64;
         ValReg = X86::RAX;
         Val = (Val << 32) | Val;
@@ -130,12 +130,12 @@ X86SelectionDAGInfo::EmitTargetCodeForMemset(SelectionDAG &DAG, DebugLoc dl,
     InFlag = Chain.getValue(1);
   }
 
-  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->is64Bit() ? X86::RCX :
-                                                              X86::ECX,
+  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->isLP64() ? X86::RCX :
+                                                             X86::ECX,
                             Count, InFlag);
   InFlag = Chain.getValue(1);
-  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->is64Bit() ? X86::RDI :
-                                                              X86::EDI,
+  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->isLP64() ? X86::RDI :
+                                                             X86::EDI,
                             Dst, InFlag);
   InFlag = Chain.getValue(1);
 
@@ -212,7 +212,7 @@ X86SelectionDAGInfo::EmitTargetCodeForMemcpy(SelectionDAG &DAG, DebugLoc dl,
     AVT = MVT::i32;
   else
     // QWORD aligned
-    AVT = Subtarget->is64Bit() ? MVT::i64 : MVT::i32;
+    AVT = Subtarget->isLP64() ? MVT::i64 : MVT::i32;
 
   unsigned UBytes = AVT.getSizeInBits() / 8;
   unsigned CountVal = SizeVal / UBytes;
@@ -220,16 +220,16 @@ X86SelectionDAGInfo::EmitTargetCodeForMemcpy(SelectionDAG &DAG, DebugLoc dl,
   unsigned BytesLeft = SizeVal % UBytes;
 
   SDValue InFlag(0, 0);
-  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->is64Bit() ? X86::RCX :
-                                                              X86::ECX,
+  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->isLP64() ? X86::RCX :
+                                                             X86::ECX,
                             Count, InFlag);
   InFlag = Chain.getValue(1);
-  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->is64Bit() ? X86::RDI :
-                                                              X86::EDI,
+  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->isLP64() ? X86::RDI :
+                                                             X86::EDI,
                             Dst, InFlag);
   InFlag = Chain.getValue(1);
-  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->is64Bit() ? X86::RSI :
-                                                              X86::ESI,
+  Chain  = DAG.getCopyToReg(Chain, dl, Subtarget->isLP64() ? X86::RSI :
+                                                             X86::ESI,
                             Src, InFlag);
   InFlag = Chain.getValue(1);
 
diff --git a/lib/Target/X86/X86Subtarget.h b/lib/Target/X86/X86Subtarget.h
index 1af585f..4aca164 100644
--- a/lib/Target/X86/X86Subtarget.h
+++ b/lib/Target/X86/X86Subtarget.h
@@ -236,6 +236,12 @@ public:
   // support are Darwin and Windows. Just use "not those".
   bool isTargetELF() const { return TargetTriple.isOSBinFormatELF(); }
   bool isTargetLinux() const { return TargetTriple.getOS() == Triple::Linux; }
+  bool isX32() const {
+    return is64Bit() && (TargetTriple.getEnvironment() == Triple::X32);
+  }
+  bool isLP64() const {
+    return is64Bit() && (TargetTriple.getEnvironment() != Triple::X32);
+  }
   bool isTargetNaCl() const {
     return TargetTriple.getOS() == Triple::NativeClient;
   }
diff --git a/lib/Target/X86/X86TargetMachine.cpp b/lib/Target/X86/X86TargetMachine.cpp
index c066a56..ae864c2 100644
--- a/lib/Target/X86/X86TargetMachine.cpp
+++ b/lib/Target/X86/X86TargetMachine.cpp
@@ -59,7 +59,10 @@ X86_64TargetMachine::X86_64TargetMachine(const Target &T, StringRef TT,
                                          Reloc::Model RM, CodeModel::Model CM,
                                          CodeGenOpt::Level OL)
   : X86TargetMachine(T, TT, CPU, FS, Options, RM, CM, OL, true),
-    DataLayout("e-p:64:64-s:64-f64:64:64-i64:64:64-f80:128:128-f128:128:128-"
+    DataLayout(getSubtargetImpl()->isX32() ?
+               "e-p:32:32-s:64-f64:64:64-i64:64:64-f80:128:128-f128:128:128-"
+               "n8:16:32:64-S128" :
+               "e-p:64:64-s:64-f64:64:64-i64:64:64-f80:128:128-f128:128:128-"
                "n8:16:32:64-S128"),
     InstrInfo(*this),
     TSInfo(*this),
diff --git a/utils/TableGen/EDEmitter.cpp b/utils/TableGen/EDEmitter.cpp
index fe484ca..95ed304 100644
--- a/utils/TableGen/EDEmitter.cpp
+++ b/utils/TableGen/EDEmitter.cpp
@@ -315,6 +315,7 @@ static int X86TypeFromOpName(LiteralConstantEmitter *type,
   LEA("lea32mem");
   LEA("lea64_32mem");
   LEA("lea64mem");
+  LEA("leax32_64mem");
 
   // all I
   PCR("i16imm_pcrel");
diff --git a/utils/TableGen/X86RecognizableInstr.cpp b/utils/TableGen/X86RecognizableInstr.cpp
index afb25be..9bdac07 100644
--- a/utils/TableGen/X86RecognizableInstr.cpp
+++ b/utils/TableGen/X86RecognizableInstr.cpp
@@ -1127,6 +1127,7 @@ OperandType RecognizableInstr::typeFromString(const std::string &s,
   TYPE("lea32mem",            TYPE_LEA)
   TYPE("lea64_32mem",         TYPE_LEA)
   TYPE("lea64mem",            TYPE_LEA)
+  TYPE("leax32_64mem",        TYPE_LEA)
   TYPE("VR64",                TYPE_MM64)
   TYPE("i64imm",              TYPE_IMMv)
   TYPE("opaque32mem",         TYPE_M1616)
@@ -1243,6 +1244,7 @@ OperandEncoding RecognizableInstr::memoryEncodingFromString
   ENCODING("lea32mem",        ENCODING_RM)
   ENCODING("lea64_32mem",     ENCODING_RM)
   ENCODING("lea64mem",        ENCODING_RM)
+  ENCODING("leax32_64mem",    ENCODING_RM)
   ENCODING("opaque32mem",     ENCODING_RM)
   ENCODING("opaque48mem",     ENCODING_RM)
   ENCODING("opaque80mem",     ENCODING_RM)
-- 
1.7.9.5

